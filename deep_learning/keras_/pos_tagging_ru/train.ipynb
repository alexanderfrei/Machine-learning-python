{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes, n_chars, max_len = 60, 35, 37\n",
    "n_val = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dropout, Input, Dense, CuDNNLSTM, Bidirectional, concatenate, Reshape\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import gc\n",
    "import pickle \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3039624, 60)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('X.pkl', 'rb') as f: X = pickle.load(f)\n",
    "with open('y.pkl', 'rb') as f: y = pickle.load(f)\n",
    "\n",
    "assert len(X) == y.shape[0]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding..\n",
      "shuffling..\n",
      "splitting..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2939624, 37), (100000, 37, 35), (2939624, 60), (100000, 60))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding\n",
    "print(\"padding..\")\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "# shuffle\n",
    "print(\"shuffling..\")\n",
    "idx = np.arange(X.shape[0])\n",
    "np.random.seed(100)\n",
    "np.random.shuffle(idx)\n",
    "X, y = X[idx], y[idx]\n",
    "\n",
    "# splitting\n",
    "print(\"splitting..\")\n",
    "X_train, X_valid = X[:-n_val], X[-n_val:]\n",
    "y_train, y_valid = y[:-n_val], y[-n_val:]\n",
    "\n",
    "# to categorical valid\n",
    "X_valid = to_categorical(X_valid, num_classes=n_chars)\n",
    "\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input -> LSTM layer -> Dropout -> FC layer /sigmoid activation/ (multilabel)\n",
    "\n",
    "def simple_lstm_model():   \n",
    "    sequence_input = Input(shape=(max_len, n_chars, ))\n",
    "    x = CuDNNLSTM(256)(sequence_input)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(n_classes, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def bidir_lstm_model():   \n",
    "    sequence_input = Input(shape=(max_len, n_chars, ))\n",
    "    x = Bidirectional(CuDNNLSTM(128))(sequence_input)\n",
    "    x = Dropout(0.2)(x)   \n",
    "    preds = Dense(n_classes, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def bidir_lstm_conv_model():   \n",
    "    # input\n",
    "    sequence_input = Input(shape=(max_len, n_chars, ))\n",
    "    \n",
    "    # Bidirectional LSTM\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(sequence_input)\n",
    "    x = Dropout(0.2)(x)   \n",
    "    \n",
    "    # convolutions\n",
    "    conv_kernel_2 = Conv1D(64, kernel_size = 2, padding = \"same\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    conv_kernel_3 = Conv1D(64, kernel_size = 3, padding = \"same\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    x = concatenate([conv_kernel_2, conv_kernel_3])\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # pooling \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool])\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # sigmoid fc layer\n",
    "    preds = Dense(n_classes, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=100000):\n",
    "    \n",
    "    X_copy = np.array(X)\n",
    "    y_copy = np.array(y)\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield to_categorical(X_copy[i:i+batch_size, :], num_classes=n_chars), y_copy[i:i+batch_size], i+batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 1\n",
    "\n",
    "# model = bidir_lstm_conv_model()\n",
    "# model = simple_lstm_model()\n",
    "model = bidir_lstm_model()\n",
    "print(model.summary())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('*'*10, 'EPOCH {}/{}'.format(str(epoch+1), str(epochs)), '*'*10)\n",
    "    batches = batch_generator(X_train, y_train, 300000)\n",
    "    for batch in batches:\n",
    "        X_batch, y_batch, i = batch\n",
    "        print('*'*5, 'BATCH {}/{}'.format(str(min(i, X_train.shape[0])), \n",
    "                                          str(X_train.shape[0])))\n",
    "        checkpoint = ModelCheckpoint(os.path.join(MODEL_DIR, \n",
    "                                                  \"model-epoch{:d}-batch{:d}k.h5\".format(epoch+1, i//1000)))\n",
    "        \n",
    "        model.fit(X_batch, y_batch, batch_size=batch_size, epochs=1, \n",
    "                  validation_data=(X_valid, y_valid), \n",
    "                  verbose=1, callbacks=[checkpoint]\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('char2int.pkl', 'rb') as f: char2int = pickle.load(f)\n",
    "with open('int2pos.pkl', 'rb') as f: int2pos = pickle.load(f)\n",
    "model = load_model(os.path.join(MODEL_DIR, 'model-epoch1-batch3000k.h5'))\n",
    "\n",
    "def predict(word):\n",
    "\n",
    "    s = word.upper() \n",
    "    X_pred = [char2int[c] for c in s]\n",
    "    X_pred = pad_sequences([X_pred], maxlen=max_len)\n",
    "    X_pred = to_categorical(X_pred, num_classes=n_chars)\n",
    "    y_pred = model.predict(X_pred)[0]\n",
    "    pred = [(int2pos[p], y_pred[p]) for p in np.argsort(y_pred)[::-1][:3]]\n",
    "    \n",
    "    print('word:', word)\n",
    "    for p, pr in pred:\n",
    "        print('{}: {:.2f}'.format(p, pr))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in ['небо','стол','мебель','добро','слава','комсомол',\n",
    "          'дятел','число','олово','серебро','делать','думал','открывал',\n",
    "          'мышь', 'слышь', 'крыша', 'мастер', 'крипта', 'валюта', 'солнце', 'тролль'\n",
    "         ]:\n",
    "    predict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ideas: \n",
    "# validate only on short words\n",
    "# validate only on most frequent words\n",
    "# train only on short words\n",
    "# cut down the list of POS, union to `main groups`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
