{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import lightgbm as lg\n",
    "\n",
    "import gc \n",
    "#from copy import deepcopy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from avito_functions import *\n",
    "from avito_classes import TargetEncoder\n",
    "\n",
    "#from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix, load_npz\n",
    "from scipy import sparse\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import scipy \n",
    "\n",
    "import time \n",
    "import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = pd.read_csv('../input/new_feautures.csv').iloc[:, 1:]\n",
    "# new_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load df\n",
      "Load agg input\n"
     ]
    }
   ],
   "source": [
    "# input \n",
    "\n",
    "data_keys = ['train', 'valid', 'holdout', 'fulltrain', 'test']\n",
    "\n",
    "print('Load df')\n",
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "print('Load agg input')\n",
    "with open('../input/map_dict.pkl', 'rb') as file: map_dict = pickle.load(file)\n",
    "#with open('../input/text_features.pkl', 'rb') as f: X_text = pickle.load(f)\n",
    "# with open('../input/tfidf_1.pkl', 'rb') as file: tfidf_dict = pickle.load(file)\n",
    "\n",
    "# with open('../input/text_num_features_clean.pkl', 'rb') as f: X_text_num = pickle.load(f)\n",
    "with open('../input/text_num_features_lemm.pkl', 'rb') as f: X_text_num = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = load_fe('sgd2')\n",
    "ext = load_fe('extra')\n",
    "# rnn = load_fe('rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df_train, df_test, map_dict, add_features=None):\n",
    "\n",
    "    print('run preprocessing..')\n",
    "    \n",
    "    target = 'deal_probability'\n",
    "\n",
    "    # get labels, merge \n",
    "    y = df_train[target].values.squeeze()\n",
    "    X = df_train.drop([target], 1).append(df_test)\n",
    "    X.index = np.arange(X.shape[0])\n",
    "\n",
    "    # map additional information\n",
    "    X['salaries'] = X.region.map(map_dict['salaries'])\n",
    "    X['population'] = X.city.map(map_dict['population'])\n",
    "\n",
    "    # merge additional features\n",
    "    if not add_features is None:\n",
    "        X = pd.concat([X, add_features], 1)\n",
    "    \n",
    "    # drop useless features \n",
    "    X = X.drop(['title', 'item_id'], 1)\n",
    "   \n",
    "    category_features = ['region', 'city', \n",
    "                         'parent_category_name', 'category_name', \n",
    "                         'param_1', 'param_2', 'param_3', \n",
    "                         'user_type', 'image_top_1', 'user_id']\n",
    "\n",
    "    return X, y, category_features\n",
    "\n",
    "\n",
    "def feature_engineering(X, category_features, factorize=False, price_bins=10):\n",
    "    \n",
    "    print('run feature engineering..')\n",
    "    new_factors = []\n",
    "    \n",
    "    # numeric transformations \n",
    "    X['user_type_num'] = pd.factorize(X['user_type'])[0]\n",
    "    X['price_log'] = np.log(X.price+0.0001)\n",
    "    X['population_log'] = np.log(X.population+0.0001)\n",
    "    X['isn_log'] = np.log(X.item_seq_number+0.0001)\n",
    "    X['price_log_div_salaries'] = X['price_log'] / X['salaries']\n",
    "    \n",
    "    # bool\n",
    "    X['price_exists'] = (~X.price.isnull()).astype(int)\n",
    "    X['image_exists'] = (~X.image.isnull()).astype(int)\n",
    "    X['descr_exists'] = (~X.description.isnull()).astype(int)\n",
    "    \n",
    "    X['population_groups'] = \\\n",
    "    ((X.population >= 1000000) * 1 + \\\n",
    "    ((X.population >= 500000) & (X.population < 1000000)) * 2 + \\\n",
    "    ((X.population >= 100000) & (X.population < 500000)) * 3 + \\\n",
    "    (X.population < 100000) * 4 - 1)     \n",
    "    \n",
    "    # date\n",
    "    dt = pd.to_datetime(X.activation_date)\n",
    "    X['weekday'] = dt.dt.weekday\n",
    "    X['free_day'] = (dt.dt.dayofweek > 5).astype(int)  \n",
    "    \n",
    "    # groups, numeric \n",
    "    X = count_group_frac(X, 'price_log', ['region', 'category_name'])\n",
    "    X = count_group_frac(X, 'price_log', ['region', 'param_1'])    \n",
    "    X = count_group_frac(X, 'price_log', ['population_groups', 'category_name'])\n",
    "    X = count_group_frac(X, 'price_log', ['population_groups', 'param_1'])\n",
    "    X = count_group_frac(X, 'price_log', ['city', 'category_name'])\n",
    "    X = count_group_frac(X, 'price_log', ['city', 'param_1'])\n",
    "    \n",
    "    # cutting\n",
    "    X['price_log_cut'] = pd.cut(X['price_log'], bins=price_bins).cat.codes\n",
    "    X['isn_log_cut'] = pd.cut(pd.Series(np.log(X.item_seq_number+0.0001)), 7).cat.codes\n",
    "\n",
    "    # features\n",
    "    new_factors += ['price_exists', 'image_exists', 'descr_exists']\n",
    "    new_factors += ['weekday', 'free_day']\n",
    "    \n",
    "    X.drop(['activation_date', 'image', 'description'], axis=1, inplace=True)\n",
    "    X.drop(['price', 'item_seq_number', 'population'], axis=1, inplace=True)\n",
    "    \n",
    "    category_features += new_factors\n",
    "    category_features += ['population_groups']\n",
    "\n",
    "    if factorize==True:\n",
    "        for f in category_features:\n",
    "            X[f] = pd.factorize(X[f])[0]\n",
    "    if factorize=='pos':\n",
    "        for f in category_features:\n",
    "            X, _ = factorize_p(X, f)\n",
    "    \n",
    "    return X, category_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = pd.concat([X_text_num, new_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run preprocessing..\n",
      "run feature engineering..\n",
      "-- count fraction price_log_x_region__category_name_frac\n",
      "-- count fraction price_log_x_region__param_1_frac\n",
      "-- count fraction price_log_x_population_groups__category_name_frac\n",
      "-- count fraction price_log_x_population_groups__param_1_frac\n",
      "-- count fraction price_log_x_city__category_name_frac\n",
      "-- count fraction price_log_x_city__param_1_frac\n"
     ]
    }
   ],
   "source": [
    "# pipeline\n",
    "n_train = df_train.shape[0]\n",
    "X, y, category_features = preprocessing(df_train, df_test, map_dict, add_features)\n",
    "category_features += ['Time_zone']\n",
    "X, category_features = feature_engineering(X, category_features, factorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.salaries.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding fit\n",
    "# oh_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "# oh_encoder.fit(X[category_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run validation splitting..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1630"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split\n",
    "X, X_test = X[:n_train], X[n_train:]\n",
    "\n",
    "x_train, x_valid, x_holdout, \\\n",
    "y_train, y_valid, y_holdout, \\\n",
    "_, _, _ = validation_split(X, y)\n",
    "    \n",
    "del df_train, df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # target encoding \n",
    "# te_groups = []\n",
    "# te_groups += [[f] for f in category_features]\n",
    "\n",
    "# te_groups += [['price_log_cut', 'category_name'], \n",
    "#               ['price_log_cut', 'region'],\n",
    "#               ['price_log_cut', 'param_1'],\n",
    "#               ['region', 'parent_category_name']\n",
    "#              ]\n",
    "\n",
    "# for group in te_groups:\n",
    "#     x_train, x_valid, x_holdout = target_encoding(x_train, y_train, x_valid, group, x_holdout)\n",
    "#     X, X_test = target_encoding(X, y, X_test, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add artificial features\n",
    "for x, k in zip([x_train, x_valid, x_holdout, X, X_test], data_keys):\n",
    "    x['sgd'] = sgd[k]\n",
    "    x['ext'] = ext[k]\n",
    "#     x['rnn'] = rnn[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def num_scaling(x_train, x_valid, x_holdout=None, num=None, mode='z', copy=True):\n",
    "    \n",
    "#     if mode=='z':\n",
    "#         scaler = StandardScaler(copy=copy)\n",
    "#     elif mode=='norm':\n",
    "#         scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#     else:\n",
    "#         pass\n",
    "    \n",
    "#     if num is None:\n",
    "#         num = x_train.columns.tolist()\n",
    "    \n",
    "#     # fit \n",
    "#     x_train[num] = scaler.fit_transform(x_train[num])\n",
    "#     # transform valid \n",
    "#     x_valid[num] = scaler.transform(x_valid[num])\n",
    "    \n",
    "#     # transform holdout\n",
    "#     if x_holdout is None:\n",
    "#         return x_train, x_valid, scaler\n",
    "#     else:\n",
    "#         x_holdout[num] = scaler.transform(x_holdout[num])\n",
    "#         return x_train, x_valid, x_holdout, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_ft = list(set(x_train.columns) - set(category_features))\n",
    "\n",
    "# ## impute \n",
    "# print('impute numeric')\n",
    "# x_train, x_valid, x_holdout, _ = num_fillna(x_train, x_valid, x_holdout, num=num_ft)\n",
    "# X, X_test, _ = num_fillna(X, X_test, num=num_ft)\n",
    "\n",
    "# ## scale\n",
    "# print('scale numeric')\n",
    "# x_train, x_valid, x_holdout, _ = num_scaling(x_train, x_valid, x_holdout, num=num_ft)\n",
    "# X, X_test, _ = num_scaling(X, X_test, num=num_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_auto_fe(X, fe_list):\n",
    "    \"\"\"\n",
    "    X -- pandas dataframe\n",
    "    \"\"\"\n",
    "    for f1, f2 in itertools.combinations(fe_list, 2):\n",
    "        X[f1+'_x_'+f2] = X[f1] * X[f2]\n",
    "        X[f1+'_div_'+f2] = X[f1] / X[f2]\n",
    "    for f in fe_list:\n",
    "        X['p2_'+f] = np.power(X[f],2)\n",
    "        \n",
    "    return X\n",
    "\n",
    "num_fe = ['salaries', 'price_log', 'population_log', 'isn_log']\n",
    "\n",
    "x_train = num_auto_fe(x_train, num_fe)\n",
    "x_valid = num_auto_fe(x_valid, num_fe)\n",
    "x_holdout = num_auto_fe(x_holdout, num_fe)\n",
    "X = num_auto_fe(X, num_fe)\n",
    "X_test = num_auto_fe(X_test, num_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_holdout\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def proj_num_on_cat(train_df, test_df, target_column, group_column):\n",
    "#     \"\"\"\n",
    "#     :param train_df: train data frame\n",
    "#     :param test_df:  test data frame\n",
    "#     :param target_column: name of numerical feature\n",
    "#     :param group_column: name of categorical feature\n",
    "#     \"\"\"\n",
    "#     train_df['row_id'] = range(train_df.shape[0])\n",
    "#     test_df['row_id'] = range(test_df.shape[0])\n",
    "#     train_df['train'] = 1\n",
    "#     test_df['train'] = 0\n",
    "#     all_df = train_df[['row_id', 'train', target_column, group_column]].append(test_df[['row_id','train',\n",
    "#                                                                                         target_column, group_column]])\n",
    "#     grouped = all_df[[target_column, group_column]].groupby(group_column)\n",
    "#     the_size = pd.DataFrame(grouped.size()).reset_index()\n",
    "#     the_size.columns = [group_column, '%s_size' % target_column]\n",
    "#     the_mean = pd.DataFrame(grouped.mean()).reset_index()\n",
    "#     the_mean.columns = [group_column, '%s_mean' % target_column]\n",
    "#     the_std = pd.DataFrame(grouped.std()).reset_index().fillna(0)\n",
    "#     the_std.columns = [group_column, '%s_std' % target_column]\n",
    "#     the_median = pd.DataFrame(grouped.median()).reset_index()\n",
    "#     the_median.columns = [group_column, '%s_median' % target_column]\n",
    "#     the_stats = pd.merge(the_size, the_mean)\n",
    "#     the_stats = pd.merge(the_stats, the_std)\n",
    "#     the_stats = pd.merge(the_stats, the_median)\n",
    "\n",
    "#     the_max = pd.DataFrame(grouped.max()).reset_index()\n",
    "#     the_max.columns = [group_column, '%s_max' % target_column]\n",
    "#     the_min = pd.DataFrame(grouped.min()).reset_index()\n",
    "#     the_min.columns = [group_column, '%s_min' % target_column]\n",
    "\n",
    "#     the_stats = pd.merge(the_stats, the_max)\n",
    "#     the_stats = pd.merge(the_stats, the_min)\n",
    "\n",
    "#     all_df = pd.merge(all_df, the_stats, how='left')\n",
    "\n",
    "#     selected_train = all_df[all_df['train'] == 1]\n",
    "#     selected_test = all_df[all_df['train'] == 0]\n",
    "#     selected_train.sort_values('row_id', inplace=True)\n",
    "#     selected_test.sort_values('row_id', inplace=True)\n",
    "#     selected_train.drop([target_column, group_column, 'row_id', 'train'], axis=1, inplace=True)\n",
    "#     selected_test.drop([target_column, group_column, 'row_id', 'train'], axis=1, inplace=True)\n",
    "\n",
    "#     selected_train, selected_test = np.array(selected_train), np.array(selected_test)\n",
    "#     #print(selected_train.shape, selected_test.shape)\n",
    "#     return selected_train, selected_test\n",
    "\n",
    "# proj_num = ['salaries', 'price_log', 'population', 'isn_log', 'sgd', 'ext']\n",
    "# proj_cat = ['image_top_1', 'city', 'param_1', 'region']\n",
    "\n",
    "# # feature aggregation valid\n",
    "# train_list, test_list = [], []\n",
    "# for t in proj_num:\n",
    "#     for g in proj_cat:\n",
    "#         if t != g:\n",
    "#             print('.', end='')\n",
    "#             s_train, s_test = proj_num_on_cat(x_train, x_valid, target_column=t, group_column=g)\n",
    "#             train_list.append(s_train)\n",
    "#             test_list.append(s_test)\n",
    "            \n",
    "# train_proj = csr_matrix(np.hstack(train_list))\n",
    "# valid_proj = csr_matrix(np.hstack(test_list))\n",
    "# print('valid done')    \n",
    "\n",
    "# # feature aggregation full\n",
    "# train_list, test_list = [], []\n",
    "# for t in proj_num:\n",
    "#     for g in proj_cat:\n",
    "#         if t != g:\n",
    "#             print('.', end='')\n",
    "#             s_train, s_test = proj_num_on_cat(X, X_test, target_column=t, group_column=g)\n",
    "#             train_list.append(s_train)\n",
    "#             test_list.append(s_test)\n",
    "            \n",
    "# fulltrain_proj = csr_matrix(np.hstack(train_list))\n",
    "# test_proj = csr_matrix(np.hstack(test_list))\n",
    "# print('fulltrain done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "# for x in [x_train, x_valid, X, X_test]:\n",
    "#     x.drop(['row_id', 'train'], 1, inplace=True)\n",
    "\n",
    "# print(x_train.shape, x_valid.shape, X.shape, X_test.shape)\n",
    "\n",
    "# cols = x_train.columns.tolist() + ['proj'+str(i) for i in range(1, train_proj.shape[1]+1)]\n",
    "cols = x_train.columns.tolist()\n",
    "print(len(cols))\n",
    "# train_proj, valid_proj, fulltrain_proj, test_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103424, 68) (300000, 68) (1503424, 68) (508438, 68)\n"
     ]
    }
   ],
   "source": [
    "# x_train = hstack([x_train.values, train_proj])\n",
    "# x_valid = hstack([x_valid.values, valid_proj])\n",
    "# X = hstack([X.values, fulltrain_proj])\n",
    "# X_test = hstack([X_test.values, test_proj])\n",
    "\n",
    "print(x_train.shape, x_valid.shape, X.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# russian_stop = set(stopwords.words('russian'))\n",
    "\n",
    "# def get_col(col_name): return lambda x: x[col_name]\n",
    "# def tfidf_pipeline(max_features=[20000, 10000, 10000], \n",
    "#                    min_df=10, max_df=.9, sub_tf=True, smooth_idf=False,\n",
    "#                    ngram_range = (1, 2), stop=russian_stop\n",
    "#                   ):\n",
    "\n",
    "#     tfidf_param = {\n",
    "#         \"stop_words\": stop,\n",
    "#         \"analyzer\": 'word',\n",
    "#         \"token_pattern\": r'\\w{1,}',\n",
    "#         \"sublinear_tf\": sub_tf,\n",
    "#         \"dtype\": np.float32,\n",
    "#         \"norm\": 'l2',\n",
    "#         \"min_df\": min_df,\n",
    "#         \"max_df\": max_df,\n",
    "#         \"smooth_idf\": smooth_idf\n",
    "#     }\n",
    "    \n",
    "#     vectorizer = FeatureUnion([\n",
    "#             ('description',TfidfVectorizer(\n",
    "#                 ngram_range=ngram_range,\n",
    "#                 max_features=max_features[0],\n",
    "#                 **tfidf_param,\n",
    "#                 preprocessor=get_col('description'))),\n",
    "#             ('text_feat',CountVectorizer(\n",
    "#                 ngram_range=ngram_range,\n",
    "#                 max_features=max_features[1],\n",
    "#                 min_df=min_df,\n",
    "#                 preprocessor=get_col('text_feat'))),\n",
    "#             ('title',TfidfVectorizer(\n",
    "#                 ngram_range=ngram_range,\n",
    "#                 max_features=max_features[2],\n",
    "#                 **tfidf_param,\n",
    "#                 preprocessor=get_col('title')))\n",
    "#         ])\n",
    "    \n",
    "#     # tfidf validation\n",
    "#     start_vect=time.time()\n",
    "\n",
    "#     train = vectorizer.fit_transform(x_train_text.to_dict('records'))\n",
    "#     val = vectorizer.transform(x_valid_text.to_dict('records'))\n",
    "#     tfvocab = vectorizer.get_feature_names()\n",
    "\n",
    "#     print(\"Train shape:\", train.shape)\n",
    "#     print(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))\n",
    "#     start_vect=time.time()\n",
    "\n",
    "#     cols = x_train.columns.tolist() + tfvocab\n",
    "#     train = hstack([csr_matrix(x_train.values), \n",
    "#                                  train])\n",
    "#     valid = hstack([csr_matrix(x_valid.values), \n",
    "#                                  val])\n",
    "#     ds_train = lg.Dataset(train, y_train, feature_name=cols, categorical_feature=category_features)\n",
    "#     ds_valid = lg.Dataset(valid, y_valid, feature_name=cols, categorical_feature=category_features)\n",
    "\n",
    "#     model, evals_results = lg_train(lg_params, ds_train, ds_valid, 1000, 50,\n",
    "#                                     verbose_eval=1000)\n",
    "\n",
    "#     print(\"Train Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))\n",
    "#     return evals_results['valid']['rmse'][model.best_iteration-1]\n",
    "\n",
    "# lg_params = {   \n",
    "#     'nthread': 4,\n",
    "#     'objective': 'regression',\n",
    "#     'metric': 'rmse',\n",
    "#     'learning_rate': 0.2,\n",
    "#     'num_leaves': 200, \n",
    "#     'subsample': 0.75, \n",
    "#     'colsample_bytree': 0.6,\n",
    "#     'min_child_weight': 20,    \n",
    "# }\n",
    "\n",
    "# # TEXT pipeline\n",
    "\n",
    "# with open('../input/text_features_clean.pkl', 'rb') as f: X_text = pickle.load(f)\n",
    "# x_train_text, x_valid_text, x_holdout_text, \\\n",
    "# _, _, _, \\\n",
    "# _, _, _ = validation_split(X_text[:X.shape[0]], y)\n",
    "\n",
    "# grid_params = [\n",
    "#     {'min_df': 10, 'max_features': [100000, None, None]},\n",
    "# ]\n",
    "# grid_error = []\n",
    "# for param in grid_params:\n",
    "#     print(\"-- param:\", param)\n",
    "#     error = tfidf_pipeline(**param)\n",
    "#     grid_error.append((param, error))\n",
    "#     print(\"error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ TFIDF grid ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# stop = set(stopwords.words('russian'))\n",
    "# min_df = 10\n",
    "# max_df = .9\n",
    "# smooth_idf = False\n",
    "# sub_tf = True\n",
    "# ngram_range = (1,2)\n",
    "# max_features = [100000, None, None]\n",
    "\n",
    "# tfidf_param = {\n",
    "#     \"stop_words\": stop,\n",
    "#     \"analyzer\": 'word',\n",
    "#     \"token_pattern\": r'\\w{1,}',\n",
    "#     \"sublinear_tf\": sub_tf,\n",
    "#     \"dtype\": np.float32,\n",
    "#     \"norm\": 'l2',\n",
    "#     \"min_df\": min_df,\n",
    "#     \"max_df\": max_df,\n",
    "#     \"smooth_idf\": smooth_idf\n",
    "# }\n",
    "\n",
    "# def get_col(col_name): return lambda x: x[col_name]\n",
    "\n",
    "# vectorizer = FeatureUnion([\n",
    "#         ('description',TfidfVectorizer(\n",
    "#             ngram_range=ngram_range,\n",
    "#             max_features=max_features[0],\n",
    "#             **tfidf_param,\n",
    "#             preprocessor=get_col('description'))),\n",
    "#         ('text_feat',CountVectorizer(\n",
    "#             ngram_range=ngram_range,\n",
    "#             max_features=max_features[1],\n",
    "#             min_df=min_df,\n",
    "#             preprocessor=get_col('text_feat'))),\n",
    "#         ('title',TfidfVectorizer(\n",
    "#             ngram_range=ngram_range,\n",
    "#             max_features=max_features[2],\n",
    "#             **tfidf_param,\n",
    "#             preprocessor=get_col('title')))\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../input/text_features_clean.pkl', 'rb') as f: X_text = pickle.load(f)\n",
    "# with open('../input/text_features_lemm.pkl', 'rb') as f: X_text = pickle.load(f)\n",
    "    \n",
    "# x_train_text, x_valid_text, x_holdout_text, \\\n",
    "# _, _, _, \\\n",
    "# _, _, _ = validation_split(X_text[:X.shape[0]], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tfidf validation\n",
    "# print('tfidf train')\n",
    "# start_vect=time.time()\n",
    "\n",
    "# x_train_text = vectorizer.fit_transform(x_train_text.to_dict('records'))\n",
    "# x_valid_text = vectorizer.transform(x_valid_text.to_dict('records'))\n",
    "# x_holdout_text = vectorizer.transform(x_holdout_text.to_dict('records'))\n",
    "# tfvocab = vectorizer.get_feature_names()\n",
    "\n",
    "# print(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))\n",
    "\n",
    "# # tfidf full train\n",
    "# print('tfidf full train')\n",
    "# X_text_ = vectorizer.fit_transform(X_text[:X.shape[0]].to_dict('records'))\n",
    "# X_test_text = vectorizer.transform(X_text[X.shape[0]:].to_dict('records'))\n",
    "# tfvocab_full = vectorizer.get_feature_names()  \n",
    "\n",
    "# X_text = X_text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_dict = {}\n",
    "# tfidf_dict['train'] = x_train_text\n",
    "# tfidf_dict['valid'] = x_valid_text\n",
    "# tfidf_dict['holdout'] = x_holdout_text\n",
    "# tfidf_dict['tfvocab'] = tfvocab\n",
    "\n",
    "# tfidf_dict['fulltrain'] = X_text\n",
    "# tfidf_dict['test'] = X_test_text\n",
    "# tfidf_dict['tfvocab_full'] = tfvocab_full\n",
    "\n",
    "# with open('../input/tfidf_2.pkl', 'wb') as file: pickle.dump(file=file, obj=tfidf_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tfidf features \n",
    "    \n",
    "with open('../input/tfidf_2.pkl', 'rb') as file: tfidf_dict=pickle.load(file=file)\n",
    "    \n",
    "x_train_text = tfidf_dict['train']\n",
    "x_valid_text = tfidf_dict['valid']\n",
    "x_holdout_text = tfidf_dict['holdout']\n",
    "tfvocab = tfidf_dict['tfvocab']\n",
    "cols_train = cols + tfvocab\n",
    "\n",
    "x_train = hstack([csr_matrix(x_train.values), x_train_text])\n",
    "x_valid = hstack([csr_matrix(x_valid.values), x_valid_text])\n",
    "x_holdout = hstack([csr_matrix(x_holdout.values), x_holdout_text])\n",
    "\n",
    "# x_train = hstack([x_train, x_train_text])\n",
    "# x_valid = hstack([x_valid, x_valid_text])\n",
    "# x_holdout = hstack([x_holdout, x_holdout_text])\n",
    "\n",
    "del x_train_text, x_valid_text, x_holdout_text\n",
    "\n",
    "### full \n",
    "\n",
    "X_text = tfidf_dict['fulltrain']\n",
    "X_test_text = tfidf_dict['test']\n",
    "tfvocab_full = tfidf_dict['tfvocab_full']\n",
    "\n",
    "cols_fulltrain = cols + tfvocab_full\n",
    "\n",
    "X = hstack([csr_matrix(X.values), X_text])\n",
    "X_test = hstack([csr_matrix(X_test.values), X_test_text])\n",
    "\n",
    "# X = hstack([X, X_text])\n",
    "# X_test = hstack([X_test, X_test_text])\n",
    "del X_text, X_test_text\n",
    "\n",
    "# del tfidf_dict\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lg_pipeline(params):    \n",
    "#     num_boost_rounds = 2000\n",
    "#     early_stopping_rounds = 20\n",
    "#     verbose = 50\n",
    "#     model, evals_results = lg_train(params, ds_train, ds_valid, \n",
    "#                              num_boost_rounds, early_stopping_rounds, verbose_eval=verbose)\n",
    "#     error = evals_results['valid']['rmse'][model.best_iteration-1]\n",
    "#     h_error = rmse(y_holdout, model.predict(x_holdout))\n",
    "#     return float(str(np.round(error, 5))), float(str(np.round(h_error, 5)))\n",
    "\n",
    "# lg_params = {\n",
    "#     'nthread': 2,\n",
    "#     'objective': 'regression',\n",
    "#     'metric': 'rmse',\n",
    "#     'learning_rate': 0.2,\n",
    "#     'num_leaves': 127,\n",
    "#     'max_depth': 0,\n",
    "#     'subsample': 0.95, \n",
    "#     'bagging_freq': 1,\n",
    "#     'feature_fraction': 0.4,\n",
    "    \n",
    "#     'min_child_weight': 10,\n",
    "#     'lambda_l1': 2,\n",
    "    \n",
    "#     'cat_l2': 20,\n",
    "#     'cat_smooth': 50,\n",
    "#     'min_data_per_group': 100\n",
    "# }\n",
    "\n",
    "# grid_params = [\n",
    "#     {'lambda_l1':3},\n",
    "# ]\n",
    "\n",
    "# grid_error = []\n",
    "# for param in grid_params:\n",
    "#     start_vect=time.time()\n",
    "#     print(\"-- param:\", param)\n",
    "#     # train lgbm\n",
    "#     default = lg_params.copy()\n",
    "#     default.update(**param)\n",
    "#     error, h_error = lg_pipeline(default)\n",
    "#     runtime = float(str(np.round((time.time() - start_vect)/60, 1))) \n",
    "#     # save & print\n",
    "#     grid_error.append((param, error, h_error, runtime))\n",
    "#     print(\"  error:\", error)\n",
    "#     print(\"h error:\", h_error)\n",
    "#     print(\"Train Runtime: %0.1f Minutes\"%(runtime))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ GRID SEARCH LGBM ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103424, 144630)\n"
     ]
    }
   ],
   "source": [
    "assert X.shape[1] == X_test.shape[1]\n",
    "assert x_train.shape[1] == x_valid.shape[1]\n",
    "# assert x_train.shape[1] == x_holdout.shape[1]\n",
    "assert X_test.shape[1] == X.shape[1]\n",
    "print(x_train.shape)\n",
    "\n",
    "# category_features = []\n",
    "\n",
    "ds_train = lg.Dataset(x_train, y_train, feature_name=cols_train, categorical_feature=category_features)\n",
    "ds_valid = lg.Dataset(x_valid, y_valid, feature_name=cols_train, categorical_feature=category_features)\n",
    "\n",
    "# ds_train = lg.Dataset(x_train, y_train, feature_name=cols)\n",
    "# ds_valid = lg.Dataset(x_valid, y_valid, feature_name=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-06-27 17:23:05.896776'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lg_train(lg_params, xgtrain, xgvalid, num_boost_round, early_stopping_rounds, callbacks=None, verbose_eval=10):\n",
    "    evals_results = {}\n",
    "    bst = lg.train(lg_params,                      \n",
    "                   xgtrain, \n",
    "                   valid_sets=[xgtrain, xgvalid], \n",
    "                   valid_names=['train', 'valid'], \n",
    "                   evals_result=evals_results, \n",
    "                   num_boost_round=num_boost_round,\n",
    "                   early_stopping_rounds=early_stopping_rounds,\n",
    "                   verbose_eval=verbose_eval,\n",
    "                   callbacks = callbacks\n",
    "                  )\n",
    "    return bst, evals_results\n",
    "\n",
    "num_boost_rounds = 5000\n",
    "# lr_decay = [0.02] * 10 + [0.2] * 90 + [0.1] * 600 + [0.05] * 1300 + [0.02] * 500\n",
    "lr_decay = [0.025] * num_boost_rounds\n",
    "callbacks = [lg.reset_parameter(learning_rate = lr_decay)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_params = {\n",
    "    'nthread': 4,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.025,\n",
    "    'num_leaves': 250,\n",
    "    'max_depth': 0,\n",
    "    'subsample': 0.9, \n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.3,\n",
    "    \n",
    "    'min_child_weight': 10,\n",
    "    'lambda_l1': 2,\n",
    "    \n",
    "    'cat_l2': 10,\n",
    "    'cat_smooth': 50,\n",
    "    'min_data_per_group': 50,\n",
    "    \n",
    "    'seed': 0\n",
    "}\n",
    "\n",
    "# early_stopping_rounds = 50\n",
    "# model, evals_results = lg_train(lg_params, ds_train, ds_valid, num_boost_rounds, early_stopping_rounds,\n",
    "#                                 verbose_eval=50, callbacks=callbacks)\n",
    "\n",
    "# print(\"bst.best_iteration: \", model.best_iteration)\n",
    "# print(evals_results['valid']['rmse'][model.best_iteration-1])\n",
    "\n",
    "# str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"test error:\", rmse(y_holdout, model.predict(x_holdout)))\n",
    "# lg.plot_importance(model, figsize=(12, 15), max_num_features=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del ds_train, ds_valid, x_train, x_valid\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for blending \n",
    "# blending = {}\n",
    "# blending['valid'] = model.predict(x_valid).clip(0, 1)\n",
    "# blending['holdout'] = model.predict(x_holdout).clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain's rmse: 0.214871\n",
      "[400]\ttrain's rmse: 0.210102\n",
      "[600]\ttrain's rmse: 0.206792\n",
      "[800]\ttrain's rmse: 0.204097\n",
      "[1000]\ttrain's rmse: 0.201598\n",
      "[1200]\ttrain's rmse: 0.199288\n",
      "[1400]\ttrain's rmse: 0.197171\n",
      "[1600]\ttrain's rmse: 0.195104\n",
      "[1800]\ttrain's rmse: 0.19316\n",
      "[2000]\ttrain's rmse: 0.191287\n",
      "[2200]\ttrain's rmse: 0.189559\n",
      "[2400]\ttrain's rmse: 0.187877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2018-06-27 23:24:24.909337'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model on full data\n",
    "print(str(datetime.datetime.now()))\n",
    "lr_decay = [0.025] * 2500\n",
    "callbacks = [lg.reset_parameter(learning_rate = lr_decay)]\n",
    "\n",
    "ds_train_full = lg.Dataset(X, y, feature_name=cols_fulltrain, categorical_feature=category_features)\n",
    "full_train_model = lg.train(lg_params, ds_train_full, num_boost_round=2500, \n",
    "                            verbose_eval=200, valid_sets=[ds_train_full], valid_names=['train'], \n",
    "                            callbacks=callbacks\n",
    "                           )\n",
    "str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blending['test'] = full_train_model.predict(X_test).clip(0, 1)\n",
    "# with open('../blending/lg18.pkl', 'wb') as f: pickle.dump(obj=blending, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitted lg22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2018-06-27 23:35:08.607499'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_name = 'lg22'\n",
    "\n",
    "import os \n",
    "if not os.path.exists('../sub'):\n",
    "    os.mkdir('../sub')\n",
    "\n",
    "df_sample = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "df_sample.deal_probability = full_train_model.predict(X_test).clip(0, 1)\n",
    "df_sample.to_csv('../sub/' + sub_name + '.csv', index=False)\n",
    "\n",
    "print('submitted', sub_name)\n",
    "str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
