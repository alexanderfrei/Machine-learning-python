{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import gc \n",
    "\n",
    "from avito_functions import * \n",
    "from avito_classes import TargetEncoder\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix, vstack\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load df\n",
      "Load agg input\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "\n",
    "new_features = pd.read_csv('../input/new_feautures.csv').iloc[:, 1:]\n",
    "data_keys = ['train', 'valid', 'holdout', 'fulltrain', 'test']\n",
    "\n",
    "print('Load df')\n",
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "print('Load agg input')\n",
    "with open('../input/map_dict.pkl', 'rb') as file: map_dict = pickle.load(file)\n",
    "#with open('../input/text_features.pkl', 'rb') as f: X_text = pickle.load(f)\n",
    "with open('../input/text_num_features.pkl', 'rb') as f: X_text_num = pickle.load(f)\n",
    "sgd = load_fe('sgd2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run preprocessing..\n",
      "run feature engineering..\n",
      "-- count fraction price_x_region__category_name_frac\n",
      "-- count fraction price_x_region__param_1_frac\n",
      "-- count fraction price_x_region__param_2_frac\n",
      "-- count fraction price_x_region__image_top_1_frac\n",
      "-- count fraction price_x_city__category_name_frac\n",
      "-- count fraction price_x_city__param_1_frac\n",
      "-- count fraction price_x_city__param_2_frac\n",
      "-- count fraction price_x_city__image_top_1_frac\n",
      "-- count fraction price_x_image_top_1__category_name_frac\n",
      "-- count fraction price_x_image_top_1__param_1_frac\n",
      "-- count fraction price_x_image_top_1__param_2_frac\n",
      "-- count fraction price_x_population_groups__param_1_frac\n",
      "-- combine factors: price_log_cut_x_parent_category_name\n",
      "-- combine factors: price_log_cut_x_category_name\n",
      "-- combine factors: price_log_cut_x_region\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2018-06-26 01:12:05.283169'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pipeline\n",
    "n_train = df_train.shape[0]\n",
    "add_features = X_text_num\n",
    "\n",
    "X, y, category_features = preprocessing(df_train, df_test, map_dict, add_features)\n",
    "X, category_features = feature_engineering(X, category_features)\n",
    "\n",
    "del df_train, df_test\n",
    "gc.collect()\n",
    "str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE\n",
    "\n",
    "# f = category_features[0]\n",
    "# dense = pd.get_dummies(X[f], prefix=f, dummy_na=True)\n",
    "# ohe_data = csr_matrix(dense)\n",
    "# ohe_features = dense.columns.tolist()\n",
    "\n",
    "# for f in category_features[1:]:\n",
    "#     print(f)\n",
    "#     dense = pd.get_dummies(X[f], prefix=f, dummy_na=True)\n",
    "#     ohe_data = hstack([ohe_data, dense])\n",
    "#     ohe_features += dense.columns.tolist()\n",
    "    \n",
    "# ohe_data = ohe_data.tocsr()\n",
    "\n",
    "##################\n",
    "# OHE: small RAM #\n",
    "##################\n",
    "\n",
    "# dict_levels = {}\n",
    "# ohe_features = []\n",
    "# for f in category_features:\n",
    "#     X, levels = factorize(X, f)\n",
    "#     dict_levels[f] = levels\n",
    "#     ohe_features += levels \n",
    "\n",
    "# ohe_data = None\n",
    "# for f in category_features:\n",
    "#     if ohe_data is None:\n",
    "#         ohe_data = onehot_vec(X, f, dict_levels)\n",
    "#     else:\n",
    "#         ohe_data = hstack([ohe_data, onehot_vec(X, f, dict_levels)]).tocsr()\n",
    "\n",
    "# factorize for lgbm\n",
    "for f in category_features:\n",
    "    X[f] = pd.factorize(X[f])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ohe_train, ohe_test = ohe_data[:n_train], ohe_data[n_train:]\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "\n",
    "del X\n",
    "# del X, ohe_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run validation splitting..\n"
     ]
    }
   ],
   "source": [
    "# split \n",
    "x_train, x_valid, x_holdout, \\\n",
    "y_train, y_valid, y_holdout, \\\n",
    "_,_,_ = validation_split(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, k in zip([x_train, x_valid, x_holdout, X_train, X_test], data_keys):\n",
    "    x['sgd'] = sgd[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- target encoding: ['region']\n",
      "-- target encoding: ['region']\n",
      "-- target encoding: ['city']\n",
      "-- target encoding: ['city']\n",
      "-- target encoding: ['parent_category_name']\n",
      "-- target encoding: ['parent_category_name']\n",
      "-- target encoding: ['category_name']\n",
      "-- target encoding: ['category_name']\n",
      "-- target encoding: ['param_1']\n",
      "-- target encoding: ['param_1']\n",
      "-- target encoding: ['param_2']\n",
      "-- target encoding: ['param_2']\n",
      "-- target encoding: ['param_3']\n",
      "-- target encoding: ['param_3']\n",
      "-- target encoding: ['user_type']\n",
      "-- target encoding: ['user_type']\n",
      "-- target encoding: ['image_top_1']\n",
      "-- target encoding: ['image_top_1']\n",
      "-- target encoding: ['price_log_cut_x_parent_category_name']\n",
      "-- target encoding: ['price_log_cut_x_parent_category_name']\n",
      "-- target encoding: ['price_log_cut_x_category_name']\n",
      "-- target encoding: ['price_log_cut_x_category_name']\n",
      "-- target encoding: ['price_log_cut_x_region']\n",
      "-- target encoding: ['price_log_cut_x_region']\n",
      "-- target encoding: ['price_exists']\n",
      "-- target encoding: ['price_exists']\n",
      "-- target encoding: ['image_exists']\n",
      "-- target encoding: ['image_exists']\n",
      "-- target encoding: ['descr_exists']\n",
      "-- target encoding: ['descr_exists']\n",
      "-- target encoding: ['weekday']\n",
      "-- target encoding: ['weekday']\n",
      "-- target encoding: ['free_day']\n",
      "-- target encoding: ['free_day']\n",
      "-- target encoding: ['population_groups']\n",
      "-- target encoding: ['population_groups']\n",
      "-- target encoding: ['price_log_cut', 'category_name']\n",
      "-- target encoding: ['price_log_cut', 'category_name']\n",
      "-- target encoding: ['price_log_cut', 'region']\n",
      "-- target encoding: ['price_log_cut', 'region']\n",
      "-- target encoding: ['price_log_cut', 'param_1']\n",
      "-- target encoding: ['price_log_cut', 'param_1']\n",
      "-- target encoding: ['region', 'parent_category_name']\n",
      "-- target encoding: ['region', 'parent_category_name']\n"
     ]
    }
   ],
   "source": [
    "# target encoding \n",
    "\n",
    "te_groups = []\n",
    "for f in category_features:\n",
    "    te_groups.append([f])\n",
    "\n",
    "te_groups += [['price_log_cut', 'category_name'], \n",
    "              ['price_log_cut', 'region'],\n",
    "              ['price_log_cut', 'param_1'],\n",
    "              ['region', 'parent_category_name']\n",
    "             ]\n",
    "\n",
    "for group in te_groups:\n",
    "    x_train, x_valid, x_holdout = target_encoding(x_train, y_train, x_valid, group, x_holdout)\n",
    "    X_train, X_test = target_encoding(X_train, y, X_test, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103424, 56) True\n",
      "(300000, 56) True\n",
      "(100000, 56) True\n",
      "(1503424, 56) True\n",
      "(508438, 56) True\n"
     ]
    }
   ],
   "source": [
    "# for x in [x_train, x_valid, x_holdout]:\n",
    "#     x.drop(category_features, 1, inplace=True)\n",
    "#     print(x.shape, all(x.columns == x_train.columns))\n",
    "\n",
    "# save category features \n",
    "cat_data = []\n",
    "for x in [x_train, x_valid, x_holdout, X_train, X_test]:\n",
    "    cat_data.append(x[category_features])\n",
    "    x.drop(category_features, 1, inplace=True)\n",
    "    print(x.shape, all(x.columns == x_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impute numeric\n",
      "scale numeric\n"
     ]
    }
   ],
   "source": [
    "## impute \n",
    "print('impute numeric')\n",
    "x_train, x_valid, x_holdout, _ = num_fillna(x_train, x_valid, x_holdout)\n",
    "X_train, X_test, _ = num_fillna(X_train, X_test)\n",
    "\n",
    "## scale\n",
    "print('scale numeric')\n",
    "x_train, x_valid, x_holdout, _ = num_scaling(x_train, x_valid, x_holdout)\n",
    "X_train, X_test, _ = num_scaling(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature union \n",
    "\n",
    "num_features = x_train.columns.tolist()\n",
    "features = num_features\n",
    "# features = num_features + ohe_features\n",
    "\n",
    "# x_train = hstack([x_train, ohe_train]).tocsr()\n",
    "# x_valid = hstack([x_valid, ohe_valid]).tocsr()\n",
    "# x_holdout = hstack([x_holdout, ohe_holdout]).tocsr()\n",
    "# X_train = hstack([X_train, ohe_data[:n_train]]).tocsr()\n",
    "# X_test = hstack([X_test, ohe_test]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric noise\n",
    "# noise_train = np.random.randn(x_train.shape[0], 20)\n",
    "# noise_valid = np.random.randn(x_valid.shape[0], 20)\n",
    "# x_train_noise = np.hstack([x_train, noise_train])\n",
    "# x_valid_noise = np.hstack([x_valid, noise_valid])\n",
    "\n",
    "# rf = RandomForestRegressor(max_depth=8,\n",
    "#                            n_estimators=250,\n",
    "#                            verbose=1, \n",
    "#                            min_samples_leaf=10,\n",
    "#                            max_features=10,\n",
    "#                            n_jobs=4\n",
    "#                           )\n",
    "# rf.fit(x_train_noise, y_train)\n",
    "# print(rmse(y_valid, rf.predict(x_valid_noise)))\n",
    "\n",
    "# random_importance = rf.feature_importances_[-20:].max()\n",
    "# mask = rf.feature_importances_ > random_importance\n",
    "\n",
    "# x_train_masked = x_train_noise[:, mask]\n",
    "# x_valid_masked = x_valid_noise[:, mask]\n",
    "# x_holdout_masked = x_holdout.loc[:, mask[:-20]]\n",
    "# X_train_masked = X_train.loc[:, mask[:-20]]\n",
    "# X_test_masked = X_test.loc[:, mask[:-20]]\n",
    "\n",
    "# useful_features = list(compress(features, mask.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tsvd(50, tfidf_dict)\n",
    "# train_tsvd(100, tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# def train_tsvd(n, tfidf_dict):\n",
    "#     print('-- tSVD:', n)\n",
    "#     ret = {}\n",
    "#     tsvd = TruncatedSVD(n_components=n, random_state=2018)\n",
    "#     ret['train'] = tsvd.fit_transform(tfidf_dict['train'])\n",
    "#     ret['valid'] = tsvd.transform(tfidf_dict['valid'])\n",
    "#     ret['holdout'] = tsvd.transform(tfidf_dict['holdout'])    \n",
    "#     ret['fulltrain'] = tsvd.fit_transform(tfidf_dict['fulltrain'])\n",
    "#     ret['test'] = tsvd.transform(tfidf_dict['test'])\n",
    "#     with open('../fe/tfidf_svd' + str(n) + '.pkl', 'wb') as file: pickle.dump(file=file, obj=ret)\n",
    "#     return ret\n",
    "\n",
    "# with open('../input/tfidf_1.pkl', 'rb') as f: \n",
    "#     tfidf_dict = pickle.load(f)\n",
    "\n",
    "# n = 20\n",
    "# fe_tfidf_svd = train_tsvd(n, tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_keys = ['train', 'valid', 'holdout', 'fulltrain', 'test']\n",
    "# data = []\n",
    "# for x, s in zip([x_train, x_valid, x_holdout, X_train, X_test], data_keys):\n",
    "#     data.append(np.hstack([x, fe_tfidf_svd[s]]))\n",
    "\n",
    "data = []\n",
    "labels = [y_train, y_valid, y_holdout]\n",
    "for x in [x_train, x_valid, x_holdout, X_train, X_test]:\n",
    "    data.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(**params)\n",
    "# rf.fit(data[3], y)\n",
    "# pred_test = rf.predict(data[4])\n",
    "# save_data = [pred_val, pred_hol, pred_test]\n",
    "# save_pred_holdout(save_data, 'rf1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  3.9min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  3.8min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  4.0min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22352+-0.00028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  5.6min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    7.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   11.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    7.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  5.6min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    7.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   10.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  5.5min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   10.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    7.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  6.8min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    9.5s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22264+-0.00022\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "params = {'max_depth':30, \n",
    "          'n_estimators':100,\n",
    "          'verbose':1, \n",
    "          'min_samples_leaf': 1,\n",
    "          'max_features':20,\n",
    "          'n_jobs':4\n",
    "         }\n",
    "\n",
    "# pred_val, pred_hol, extra = train_sklearn(ExtraTreesRegressor, params, data, labels)\n",
    "\n",
    "model = ExtraTreesRegressor(**params)\n",
    "\n",
    "# valid \n",
    "data = [x_train.values, x_valid.values, x_holdout.values]\n",
    "preds = oof_prediction(model, data, y_train)\n",
    "# test\n",
    "data = [X_train.values, X_test.values]\n",
    "preds += oof_prediction(model, data, y)\n",
    "\n",
    "d_preds = {}\n",
    "for pred, k in zip(preds, ['train', 'valid', 'holdout', 'fulltrain', 'test']):\n",
    "    d_preds[k] = pred\n",
    "    \n",
    "with open('../fe/extra.pkl', 'wb') as file: pickle.dump(file=file, obj=d_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra = ExtraTreesRegressor(**params)\n",
    "# extra.fit(data[3], y)\n",
    "# pred_test = rf.predict(data[4])\n",
    "# save_data = [pred_val, pred_hol, pred_test]\n",
    "# save_pred_holdout(save_data, 'extra1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.230253516518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.07392767,  0.42289072,  0.18770561, ...,  0.01847857,\n",
       "         0.06639792,  0.32432398]),\n",
       " SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "        fit_intercept=True, l1_ratio=0.01, learning_rate='invscaling',\n",
       "        loss='squared_loss', max_iter=100, n_iter=None,\n",
       "        penalty='elasticnet', power_t=0.25, random_state=2018, shuffle=True,\n",
       "        tol=None, verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "params = {'max_iter': 100, \n",
    "          'loss': 'squared_loss', \n",
    "          'random_state': 2018, \n",
    "          'alpha': 0.0001,\n",
    "          'penalty': 'l2',\n",
    "          'l1_ratio': 0.01\n",
    "         }\n",
    "\n",
    "train_sklearn_valid(SGDRegressor, params, tfidf_dict['train'], tfidf_dict['valid'], y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23258+-0.00036\n",
      "0.23210+-0.00025\n"
     ]
    }
   ],
   "source": [
    "model = SGDRegressor(alpha=1e-05, random_state=2018)\n",
    "\n",
    "# valid \n",
    "data = [tfidf_dict['train'], tfidf_dict['valid'],  tfidf_dict['holdout']]\n",
    "preds = oof_prediction(model, data, y_train)\n",
    "\n",
    "# test\n",
    "data = [tfidf_dict['fulltrain'], tfidf_dict['test']]\n",
    "preds += oof_prediction(model, data, y)\n",
    "\n",
    "sgd_preds = {}\n",
    "for pred, k in zip(preds, ['train', 'valid', 'holdout', 'fulltrain', 'test']):\n",
    "    sgd_preds[k] = pred\n",
    "    \n",
    "with open('../fe/sgd2.pkl', 'wb') as file: pickle.dump(file=file, obj=sgd_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
