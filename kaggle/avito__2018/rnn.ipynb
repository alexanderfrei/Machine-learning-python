{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, CuDNNGRU, Dropout, SpatialDropout1D, CuDNNLSTM, Input, concatenate\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gc \n",
    "from avito_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load df\n",
      "Load agg input\n",
      "run preprocessing..\n",
      "run feature engineering..\n",
      "-- count fraction price_x_region__category_name_frac\n",
      "-- count fraction price_x_region__param_1_frac\n",
      "-- count fraction price_x_region__param_2_frac\n",
      "-- count fraction price_x_region__image_top_1_frac\n",
      "-- count fraction price_x_city__category_name_frac\n",
      "-- count fraction price_x_city__param_1_frac\n",
      "-- count fraction price_x_city__param_2_frac\n",
      "-- count fraction price_x_city__image_top_1_frac\n",
      "-- count fraction price_x_image_top_1__category_name_frac\n",
      "-- count fraction price_x_image_top_1__param_1_frac\n",
      "-- count fraction price_x_image_top_1__param_2_frac\n",
      "-- count fraction price_x_population_groups__param_1_frac\n",
      "-- combine factors: price_log_cut_x_parent_category_name\n",
      "-- combine factors: price_log_cut_x_category_name\n",
      "-- combine factors: price_log_cut_x_region\n",
      "run validation splitting..\n",
      "-- target encoding: ['region']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\Jupyter\\avito\\scripts\\avito_classes.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp( (n - self.k) / self.f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- target encoding: ['region']\n",
      "-- target encoding: ['city']\n",
      "-- target encoding: ['city']\n",
      "-- target encoding: ['parent_category_name']\n",
      "-- target encoding: ['parent_category_name']\n",
      "-- target encoding: ['category_name']\n",
      "-- target encoding: ['category_name']\n",
      "-- target encoding: ['param_1']\n",
      "-- target encoding: ['param_1']\n",
      "-- target encoding: ['param_2']\n",
      "-- target encoding: ['param_2']\n",
      "-- target encoding: ['param_3']\n",
      "-- target encoding: ['param_3']\n",
      "-- target encoding: ['user_type']\n",
      "-- target encoding: ['user_type']\n",
      "-- target encoding: ['image_top_1']\n",
      "-- target encoding: ['image_top_1']\n",
      "-- target encoding: ['price_log_cut_x_parent_category_name']\n",
      "-- target encoding: ['price_log_cut_x_parent_category_name']\n",
      "-- target encoding: ['price_log_cut_x_category_name']\n",
      "-- target encoding: ['price_log_cut_x_category_name']\n",
      "-- target encoding: ['price_log_cut_x_region']\n",
      "-- target encoding: ['price_log_cut_x_region']\n",
      "-- target encoding: ['price_exists']\n",
      "-- target encoding: ['price_exists']\n",
      "-- target encoding: ['image_exists']\n",
      "-- target encoding: ['image_exists']\n",
      "-- target encoding: ['descr_exists']\n",
      "-- target encoding: ['descr_exists']\n",
      "-- target encoding: ['weekday']\n",
      "-- target encoding: ['weekday']\n",
      "-- target encoding: ['free_day']\n",
      "-- target encoding: ['free_day']\n",
      "-- target encoding: ['population_groups']\n",
      "-- target encoding: ['population_groups']\n",
      "-- target encoding: ['price_log_cut', 'category_name']\n",
      "-- target encoding: ['price_log_cut', 'category_name']\n",
      "-- target encoding: ['price_log_cut', 'region']\n",
      "-- target encoding: ['price_log_cut', 'region']\n",
      "-- target encoding: ['price_log_cut', 'param_1']\n",
      "-- target encoding: ['price_log_cut', 'param_1']\n",
      "-- target encoding: ['region', 'parent_category_name']\n",
      "-- target encoding: ['region', 'parent_category_name']\n",
      "(1103424, 56) True\n",
      "(300000, 56) True\n",
      "(100000, 56) True\n",
      "(1503424, 56) True\n",
      "(508438, 56) True\n",
      "impute numeric\n",
      "scale numeric\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(df_train, df_test, map_dict, add_features=None):\n",
    "\n",
    "    print('run preprocessing..')\n",
    "    \n",
    "    target = 'deal_probability'\n",
    "\n",
    "    # get labels, merge \n",
    "    y = df_train[target].values.squeeze()\n",
    "    X = df_train.drop([target], 1).append(df_test)\n",
    "    X.index = np.arange(X.shape[0])\n",
    "\n",
    "    # map additional information\n",
    "    X['salaries'] = X.region.map(map_dict['salaries'])\n",
    "    X['population'] = X.city.map(map_dict['population'])\n",
    "\n",
    "    # merge additional features\n",
    "    if not add_features is None:\n",
    "        X = pd.concat([X, add_features], 1)\n",
    "    \n",
    "    # drop useless features \n",
    "    X = X.drop(['title', 'item_id', 'user_id'], 1)\n",
    "   \n",
    "    category_features = ['region', 'city', \n",
    "                         'parent_category_name', 'category_name', \n",
    "                         'param_1', 'param_2', 'param_3', \n",
    "                         'user_type', 'image_top_1']\n",
    "\n",
    "    return X, y, category_features\n",
    "\n",
    "\n",
    "# numeric \n",
    "\n",
    "data_keys = ['train', 'valid', 'holdout', 'fulltrain', 'test']\n",
    "\n",
    "print('Load df')\n",
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "print('Load agg input')\n",
    "with open('../input/map_dict.pkl', 'rb') as file: map_dict = pickle.load(file)\n",
    "with open('../input/text_num_features_lemm.pkl', 'rb') as f: X_text_num = pickle.load(f)\n",
    "sgd = load_fe('sgd2')\n",
    "ext = load_fe('extra')\n",
    "\n",
    "n_train = df_train.shape[0]\n",
    "add_features = X_text_num\n",
    "X, y, category_features = preprocessing(df_train, df_test, map_dict, add_features)\n",
    "X, category_features = feature_engineering(X, category_features, factorize=True)\n",
    "\n",
    "X, X_test = X[:n_train], X[n_train:]\n",
    "\n",
    "x_train, x_valid, x_holdout, \\\n",
    "y_train, y_valid, y_holdout, \\\n",
    "_, _, _ = validation_split(X, y)\n",
    "\n",
    "te_groups = []\n",
    "for f in category_features:\n",
    "    te_groups.append([f])\n",
    "\n",
    "te_groups += [['price_log_cut', 'category_name'], \n",
    "              ['price_log_cut', 'region'],\n",
    "              ['price_log_cut', 'param_1'],\n",
    "              ['region', 'parent_category_name']\n",
    "             ]\n",
    "\n",
    "for group in te_groups:\n",
    "    x_train, x_valid, x_holdout = target_encoding(x_train, y_train, x_valid, group, x_holdout)\n",
    "    X, X_test = target_encoding(X, y, X_test, group)\n",
    "\n",
    "# save category features \n",
    "for x in [x_train, x_valid, x_holdout, X, X_test]:\n",
    "    x.drop(category_features, 1, inplace=True)\n",
    "    print(x.shape, all(x.columns == x_train.columns))\n",
    "    \n",
    "## impute \n",
    "print('impute numeric')\n",
    "x_train, x_valid, x_holdout, _ = num_fillna(x_train, x_valid, x_holdout)\n",
    "X, X_test, _ = num_fillna(X, X_test)\n",
    "\n",
    "## scale\n",
    "print('scale numeric')\n",
    "x_train, x_valid, x_holdout, _ = num_scaling(x_train, x_valid, x_holdout)\n",
    "X, X_test, _ = num_scaling(X, X_test)\n",
    "\n",
    "for x, k in zip([x_train, x_valid, x_holdout, X, X_test], data_keys):\n",
    "    x['sgd'] = sgd[k]\n",
    "    x['ext'] = ext[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_valid(texts, max_words, maxlen):\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)  \n",
    "    indices = np.arange(data.shape[0])\n",
    "    \n",
    "    # shuffle\n",
    "    np.random.seed(10101)\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    \n",
    "    training_samples = n_train - 400000\n",
    "    validation_samples = 300000\n",
    "    holdout_samples = 100000\n",
    "\n",
    "    X = {}\n",
    "    X['train'] = data[:training_samples]\n",
    "    X['valid'] = data[training_samples : training_samples + validation_samples]\n",
    "    X['holdo'] = data[training_samples + validation_samples :]\n",
    "    \n",
    "    return X, word_index\n",
    "\n",
    "def preprocessing_fulltrain(texts, max_words, maxlen):\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)  \n",
    "    \n",
    "    return data[:n_train], data[n_train:], word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('../input/train.csv')\n",
    "# labels = df_train['deal_probability'].values.squeeze()\n",
    "n_train = X.shape[0]\n",
    "\n",
    "# del df_train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011862"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../input/text_features_lemm.pkl', 'rb') as f: \n",
    "    df_text = pickle.load(f)\n",
    "texts = df_text.description.tolist()\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1103424, 60), (300000, 60), (1503424, 60))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text, X_test_text = texts[:n_train], texts[:n_train]\n",
    "\n",
    "maxlen = 50\n",
    "max_words = 1000\n",
    "\n",
    "X_text, word_index = preprocessing_valid(X_text, max_words, maxlen)\n",
    "print('done')\n",
    "X_text['fulltrain'], X_text['test'], word_index_full = preprocessing_fulltrain(texts, max_words, maxlen)\n",
    "print('done')\n",
    "\n",
    "X_text['train'].shape, X_text['valid'].shape, X_text['fulltrain'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776030"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index = {}\n",
    "f = open('../input/wiki.ru.vec', encoding='utf-8')\n",
    "for i, line in enumerate(f):\n",
    "    if i == 0: continue\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "    except ValueError:\n",
    "        pass\n",
    "f.close()\n",
    "len(embedding_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "       \n",
    "# def nn_simple():\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(32, activation='relu'))\n",
    "#     model.add(Dense(1, activation=None))\n",
    "#     return model\n",
    "\n",
    "       \n",
    "# def nn_gru_simple():\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "#     model.add(CuDNNGRU(128))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(1, activation=None))\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def nn_gru_simple():\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "#     model.add(CuDNNGRU(128))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(1, activation=None))\n",
    "#     return model\n",
    "\n",
    "# def nn_lstm_simple():\n",
    "    \n",
    "#     input_text = Input(shape=(maxlen,))\n",
    "#     x = Embedding(max_words, embedding_dim, input_length=maxlen)(input_text)\n",
    "#     x = CuDNNLSTM(64, return_sequences=True)(x)\n",
    "#     x = Dropout(0.1)(x)\n",
    "#     x = CuDNNLSTM(64)(x)\n",
    "#     x = Dropout(0.1)(x)\n",
    "#     x = Dense(1, activation=None)(x)\n",
    "#     regr_proba = Dense(1, activation=None)(x)\n",
    "#     model = Model(inputs=[input_text], outputs=[regr_proba])\n",
    "#     model.layers[1].set_weights([embedding_matrix])\n",
    "#     model.layers[1].trainable = False    \n",
    "#     model.summary()\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn_lstm_simple()\n",
    "# model.compile(optimizer='adam', loss=root_mean_squared_error)\n",
    "# history = model.fit(X['train'], y['train'], epochs=1, batch_size=256, \n",
    "#                     validation_data=(X['valid'], y['valid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nn_lstm_conv():\n",
    "    \n",
    "    # numeric\n",
    "    input_num = Input(shape=(58,))\n",
    "    num = Embedding(58, 100)(input_num)\n",
    "    num = GlobalMaxPooling1D()(num)\n",
    "    #num = Flatten()(num)\n",
    "    # text\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_words, embedding_dim, input_length=maxlen, \n",
    "                  weights=[embedding_matrix], \n",
    "                  trainable=False\n",
    "                 )(input_text)\n",
    "    x = CuDNNLSTM(64, return_sequences=True)(x)\n",
    "    x = Conv1D(32, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(32, 5, activation='relu')(x)\n",
    "    x_max = GlobalMaxPooling1D()(x)\n",
    "    x_avg = GlobalAveragePooling1D()(x)\n",
    "    x = concatenate([num, x_max, x_avg])\n",
    "    x = Dense(1, activation=None)(x)\n",
    "    regr_proba = Dense(1, activation=None)(x)\n",
    "    model = Model(inputs=[input_num, input_text], outputs=[regr_proba]) \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def nn_lstm_num():\n",
    "    \n",
    "    # numeric\n",
    "    input_num = Input(shape=(58,))\n",
    "    num = Embedding(58, 20)(input_num)\n",
    "    num = Flatten()(num)\n",
    "    num = Dropout(0.2)(num)\n",
    "    # text\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_words, embedding_dim, input_length=maxlen, \n",
    "                  weights=[embedding_matrix], \n",
    "                  trainable=False\n",
    "                 )(input_text)\n",
    "    x = Bidirectional(CuDNNLSTM(32))(x)\n",
    "    x = concatenate([num, x])\n",
    "    x = Dense(16, activation='tanh')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation=None)(x)\n",
    "    regr_proba = Dense(1, activation=None)(x)\n",
    "    model = Model(inputs=[input_num, input_text], outputs=[regr_proba]) \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def nn_lstm_top():\n",
    "    \n",
    "    # numeric\n",
    "    input_num = Input(shape=(20,))\n",
    "    num = Embedding(20, 20)(input_num)\n",
    "    num = Flatten()(num)\n",
    "    num = Dropout(0.2)(num)\n",
    "    # text\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_words, embedding_dim, input_length=maxlen, \n",
    "                  weights=[embedding_matrix], \n",
    "                  trainable=False\n",
    "                 )(input_text)\n",
    "    x = CuDNNLSTM(32)(x)\n",
    "    x = concatenate([num, x])\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation=None)(x)\n",
    "    regr_proba = Dense(1, activation=None)(x)\n",
    "    model = Model(inputs=[input_num, input_text], outputs=[regr_proba]) \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def nn_lstm_conv2():\n",
    "    \n",
    "    # numeric\n",
    "    input_num = Input(shape=(58,))\n",
    "    num = Embedding(58, 10)(input_num)\n",
    "    num = Flatten()(num)\n",
    "    # text\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_words, embedding_dim, input_length=maxlen, \n",
    "                  weights=[embedding_matrix], \n",
    "                  trainable=False\n",
    "                 )(input_text)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    x = Conv1D(64, 2, activation='relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(64, 3, activation='relu')(x)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    x = Conv1D(64, 4, activation='relu')(x)\n",
    "    x_max = GlobalMaxPooling1D()(x)\n",
    "    x_avg = GlobalAveragePooling1D()(x)\n",
    "    x = concatenate([num, x_max, x_avg])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(1, activation=None)(x)\n",
    "    regr_proba = Dense(1, activation=None)(x)\n",
    "    model = Model(inputs=[input_num, input_text], outputs=[regr_proba]) \n",
    "#     model.summary()\n",
    "    return model\n",
    "\n",
    "model = nn_lstm_conv2()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.columns\n",
    "# top_num = ['item_seq_number', 'salaries', 'population', 'price_log_cut',\n",
    "#        'isn_log_cut', 'region_mean', 'city_mean', 'parent_category_name_mean',\n",
    "#        'category_name_mean', 'param_1_mean', 'param_2_mean', 'param_3_mean',\n",
    "#        'user_type_mean', 'image_top_1_mean',\n",
    "#        'price_log_cut_x_parent_category_name_mean',\n",
    "#        'price_log_cut_x_category_name_mean', 'price_log_cut_x_region_mean',\n",
    "#        'population_groups_mean',\n",
    "#        'sgd', 'ext']\n",
    "\n",
    "# x_train.shape, X_text['train'].shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"../input/conv_w.h5\")\n",
    "# print(rmse(y_valid, model.predict([x_valid, X_text['valid']])))\n",
    "# print(rmse(y_holdout, model.predict([x_holdout, X_text['holdo']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss=root_mean_squared_error)\n",
    "# history = model.fit([x_train, X_text['train']], y_train,\n",
    "#                     epochs=1, batch_size=512,\n",
    "#                     validation_data=([x_valid, X_text['valid']], y_valid))\n",
    "\n",
    "# print(rmse(y_valid, model.predict([x_valid, X_text['valid']])))\n",
    "# print(rmse(y_holdout, model.predict([x_holdout, X_text['holdo']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('../input/conv_w.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../input/conv_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.zeros(10)\n",
    "# idx = [1,2,3]\n",
    "# b = np.ones((3,1))\n",
    "# a[idx] = b.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_prediction_keras(nn, data, text_dict, y, nfolds=4):\n",
    "    \n",
    "    train_pred = np.zeros(data[0].shape[0])\n",
    "    valid_pred = np.zeros(data[1].shape[0])\n",
    "    errors = np.zeros(nfolds)\n",
    "    if len(data) == 3: holdo_pred = np.zeros(data[2].shape[0])\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate( KFold(nfolds).split(data[0]) ):\n",
    "        X_train, X_test = data[0][train_idx], data[0][test_idx]\n",
    "        X_train_text, X_test_text = text_dict['train'][train_idx], text_dict['train'][test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # fit\n",
    "        model = nn()\n",
    "        #model.load_weights(\"../input/conv_w.h5\")\n",
    "        model.compile(optimizer='adam', loss=root_mean_squared_error)\n",
    "        model.fit([X_train, X_train_text], y_train, epochs=3, batch_size=512)\n",
    "        \n",
    "        # predict \n",
    "        test_pred = model.predict([X_test, X_test_text])\n",
    "        train_pred[test_idx] = test_pred.squeeze()\n",
    "        valid_pred += model.predict([data[1], text_dict['valid']]).squeeze()\n",
    "        if len(data) == 3: holdo_pred += model.predict([data[2], text_dict['holdo']]).squeeze()\n",
    "        errors[i] = rmse(y_test, test_pred)\n",
    "    \n",
    "    print(\"{:.5f}+-{:.5f}\".format(errors.mean(), errors.std()))\n",
    "    \n",
    "    valid_pred /= nfolds\n",
    "    if len(data) == 3: \n",
    "        holdo_pred /= nfolds    \n",
    "        return [train_pred, valid_pred, holdo_pred]\n",
    "    else:\n",
    "        return [train_pred, valid_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "827568/827568 [==============================] - 61s 74us/step - loss: 0.2320\n",
      "Epoch 2/3\n",
      "827568/827568 [==============================] - 59s 71us/step - loss: 0.2291\n",
      "Epoch 3/3\n",
      "827568/827568 [==============================] - 59s 71us/step - loss: 0.2278\n",
      "Epoch 1/3\n",
      "827568/827568 [==============================] - 59s 71us/step - loss: 0.2337\n",
      "Epoch 2/3\n",
      "827568/827568 [==============================] - 58s 71us/step - loss: 0.2297\n",
      "Epoch 3/3\n",
      "827568/827568 [==============================] - 59s 71us/step - loss: 0.2281\n",
      "Epoch 1/3\n",
      "827568/827568 [==============================] - 59s 72us/step - loss: 0.2318\n",
      "Epoch 2/3\n",
      "827568/827568 [==============================] - 58s 71us/step - loss: 0.2290\n",
      "Epoch 3/3\n",
      "827568/827568 [==============================] - 59s 71us/step - loss: 0.2278\n",
      "Epoch 1/3\n",
      "827568/827568 [==============================] - 59s 72us/step - loss: 0.2327\n",
      "Epoch 2/3\n",
      "827568/827568 [==============================] - 59s 71us/step - loss: 0.2324\n",
      "Epoch 3/3\n",
      "827568/827568 [==============================] - 59s 71us/step - loss: 0.2280\n",
      "0.22844+-0.00075\n"
     ]
    }
   ],
   "source": [
    "data = [x_train.values, x_valid.values, x_holdout.values]\n",
    "preds = oof_prediction_keras(nn_lstm_conv2, data, X_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_prediction_keras(nn, data, text_data, y, nfolds=4):\n",
    "    \n",
    "    train_pred = np.zeros(data[0].shape[0])\n",
    "    valid_pred = np.zeros(data[1].shape[0])\n",
    "    errors = np.zeros(nfolds)\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate( KFold(nfolds).split(data[0]) ):\n",
    "        X_train, X_test = data[0][train_idx], data[0][test_idx]\n",
    "        X_train_text, X_test_text = text_data[0][train_idx], text_data[0][test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # fit\n",
    "        model = nn()\n",
    "        #model.load_weights(\"../input/conv_w.h5\")\n",
    "        model.compile(optimizer='adam', loss=root_mean_squared_error)\n",
    "        model.fit([X_train, X_train_text], y_train, epochs=3, batch_size=512)\n",
    "        \n",
    "        # predict \n",
    "        test_pred = model.predict([X_test, X_test_text])\n",
    "        train_pred[test_idx] = test_pred.squeeze()\n",
    "        valid_pred += model.predict([data[1], text_data[1]]).squeeze()\n",
    "        errors[i] = rmse(y_test, test_pred)\n",
    "    \n",
    "    print(\"{:.5f}+-{:.5f}\".format(errors.mean(), errors.std()))\n",
    "    \n",
    "    valid_pred /= nfolds\n",
    "    return [train_pred, valid_pred]\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index_full.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1127568/1127568 [==============================] - 80s 71us/step - loss: 0.2315\n",
      "Epoch 2/3\n",
      "1127568/1127568 [==============================] - 80s 71us/step - loss: 0.2284\n",
      "Epoch 3/3\n",
      "1127568/1127568 [==============================] - 80s 71us/step - loss: 0.2271\n",
      "Epoch 1/3\n",
      "1127568/1127568 [==============================] - 81s 71us/step - loss: 0.2886\n",
      "Epoch 2/3\n",
      "1127568/1127568 [==============================] - 80s 71us/step - loss: 0.2375\n",
      "Epoch 3/3\n",
      "1127568/1127568 [==============================] - 80s 71us/step - loss: 0.2325\n",
      "Epoch 1/3\n",
      "1127568/1127568 [==============================] - 80s 71us/step - loss: 0.2317\n",
      "Epoch 2/3\n",
      "1127568/1127568 [==============================] - 80s 71us/step - loss: 0.2286\n",
      "Epoch 3/3\n",
      "1127568/1127568 [==============================] - 80s 71us/step - loss: 0.2272\n",
      "Epoch 1/3\n",
      "1127568/1127568 [==============================] - 81s 72us/step - loss: 0.2313\n",
      "Epoch 2/3\n",
      "1127568/1127568 [==============================] - 80s 71us/step - loss: 0.2386\n",
      "Epoch 3/3\n",
      "1127568/1127568 [==============================] - 80s 71us/step - loss: 0.2297\n",
      "0.22911+-0.00146\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "data = [X.values, X_test.values]\n",
    "preds += oof_prediction_keras(nn_lstm_conv2, data, [X_text['fulltrain'], X_text['test']], y)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_preds = {}\n",
    "for pred, k in zip(preds, ['train', 'valid', 'holdout', 'fulltrain', 'test']):\n",
    "    d_preds[k] = pred\n",
    "    \n",
    "with open('../fe/rnn2.pkl', 'wb') as file: pickle.dump(file=file, obj=d_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save for blending \n",
    "# blending = {}\n",
    "# blending['valid'] = model.predict(x_valid).clip(0, 1)\n",
    "# blending['holdout'] = model.predict(x_holdout).clip(0, 1)\n",
    "\n",
    "# # TODO model fit full data\n",
    "# blending['test'] = full_train_model.predict(X_test).clip(0, 1)\n",
    "# with open('../blending/lg1.pkl', 'wb') as f: pickle.dump(obj=blending, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
