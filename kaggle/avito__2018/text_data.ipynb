{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import gc \n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from avito_functions import preprocessing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input \n",
    "\n",
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2011862, 18)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_train, df_test])\n",
    "df.index = np.arange(X.shape[0])\n",
    "\n",
    "del df_train, df_test\n",
    "gc.collect()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# param\n",
    "df['text_feat'] = df.apply(lambda row: ' '.join([\n",
    "    str(row['param_1']), \n",
    "    str(row['param_2']), \n",
    "    str(row['param_3'])]),axis=1) # Group Param Features\n",
    "df.drop([\"param_1\",\"param_2\",\"param_3\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textfeats = [\"description\",\"text_feat\", \"title\"]\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "\n",
    "df = df[textfeats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description\n",
      "text_feat\n",
      "title\n"
     ]
    }
   ],
   "source": [
    "# feature engineering \n",
    "def number_of_caps(s):\n",
    "    return len([c for c in s if c.isupper()])\n",
    "\n",
    "def number_of_excl(s):\n",
    "    return len([c for c in s if c=='!'])\n",
    "\n",
    "for cols in textfeats:\n",
    "    print(cols)\n",
    "        \n",
    "    df[cols] = df[cols].astype(str)\n",
    "    df[cols] = df[cols].str.replace('nan', '')\n",
    "    df[cols] = df[cols].fillna('nicapotato')\n",
    "    #df[cols + '_num_caps'] = df[cols].apply(number_of_caps)\n",
    "    df[cols] = df[cols].str.lower()\n",
    "    \n",
    "    df[cols + '_num_excl'] = df[cols].apply(number_of_excl)\n",
    "    df[cols + '_num_chars'] = df[cols].apply(len) \n",
    "    #df[cols + '_num_caps_frac'] = df[cols + '_num_caps'] / df[cols + '_num_chars'] * 100\n",
    "    df[cols + '_num_excl_frac'] = df[cols + '_num_excl'] / df[cols + '_num_chars'] * 100\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split()))\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../input/text_num_features.pkl', 'wb') as f: pickle.dump(obj=df.iloc[:, 3:], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../input/text_features.pkl', 'wb') as f: pickle.dump(obj=df.iloc[:, :3], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"analyzer\": 'word',\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"norm\": 'l2',\n",
    "    #\"min_df\":5,\n",
    "    #\"max_df\":.9,\n",
    "    \"smooth_idf\":False\n",
    "}\n",
    "\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "\n",
    "vectorizer = FeatureUnion([\n",
    "        ('description',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=16000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('description'))),\n",
    "        ('text_feat',CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            #max_features=7000,\n",
    "            preprocessor=get_col('text_feat'))),\n",
    "        ('title',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            **tfidf_para,\n",
    "            #max_features=7000,\n",
    "            preprocessor=get_col('title')))\n",
    "    ])\n",
    "    \n",
    "start_vect=time.time()\n",
    "vectorizer.fit(df.loc[traindex,:].to_dict('records'))\n",
    "ready_df = vectorizer.transform(df.to_dict('records'))\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "print(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Modeling Stage\")\n",
    "# Combine Dense Features with Sparse Text Bag of Words Features\n",
    "X = hstack([csr_matrix(df.loc[traindex,:].values),ready_df[0:traindex.shape[0]]]) # Sparse Matrix\n",
    "testing = hstack([csr_matrix(df.loc[testdex,:].values),ready_df[traindex.shape[0]:]])\n",
    "tfvocab = df.columns.tolist() + tfvocab\n",
    "for shape in [X,testing]:\n",
    "    print(\"{} Rows and {} Cols\".format(*shape.shape))\n",
    "print(\"Feature Names Length: \",len(tfvocab))\n",
    "del df\n",
    "gc.collect();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
