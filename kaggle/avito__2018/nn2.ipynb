{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Input, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from time import time\n",
    "import datetime\n",
    "from keras.models import Model\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from scipy import sparse\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = train.append(test)\n",
    "# data.reset_index(inplace=True)\n",
    "# train_rows = train.shape[0]\n",
    "\n",
    "# feature_results = []\n",
    "\n",
    "# for target_g in ['car', 'ind', 'reg']:\n",
    "#     features = [x for x in list(data) if target_g not in x]\n",
    "#     target_list = [x for x in list(data) if target_g in x]\n",
    "#     train_fea = np.array(data[features])\n",
    "#     for target in target_list:\n",
    "#         print(target)\n",
    "#         train_label = data[target]\n",
    "#         kfold = KFold(n_splits=5, random_state=218, shuffle=True)\n",
    "#         kf = kfold.split(data)\n",
    "#         cv_train = np.zeros(shape=(data.shape[0], 1))\n",
    "#         for i, (train_fold, validate) in enumerate(kf):\n",
    "#             X_train, X_validate, label_train, label_validate = \\\n",
    "#                 train_fea[train_fold, :], train_fea[validate, :], train_label[train_fold], train_label[validate]\n",
    "#             dtrain = xgb.DMatrix(X_train, label_train)\n",
    "#             dvalid = xgb.DMatrix(X_validate, label_validate)\n",
    "#             watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "#             bst = xgb.train(params, dtrain, num_boost_round, evals=watchlist, verbose_eval=50,\n",
    "#                             early_stopping_rounds=10)\n",
    "#             cv_train[validate, 0] += bst.predict(xgb.DMatrix(X_validate), ntree_limit=bst.best_ntree_limit)\n",
    "#         feature_results.append(cv_train)\n",
    "\n",
    "# feature_results = np.hstack(feature_results)\n",
    "# train_features = feature_results[:train_rows, :]\n",
    "# test_features = feature_results[train_rows:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_valid(texts, max_words, maxlen, n_train):\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)  \n",
    "    indices = np.arange(data.shape[0])\n",
    "    \n",
    "    # shuffle\n",
    "    np.random.seed(10101)\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    \n",
    "    training_samples = n_train - 400000\n",
    "    validation_samples = 300000\n",
    "    holdout_samples = 100000\n",
    "\n",
    "    X = {}\n",
    "    X['train'] = data[:training_samples]\n",
    "    X['valid'] = data[training_samples : training_samples + validation_samples]\n",
    "    X['holdo'] = data[training_samples + validation_samples :]\n",
    "    \n",
    "    return X, word_index\n",
    "\n",
    "def preprocessing_fulltrain(texts, max_words, maxlen, n_train):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)  \n",
    "    \n",
    "    return data[:n_train], data[n_train:], word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text len: 2011862\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "with open('../input/text_features_lemm.pkl', 'rb') as f: \n",
    "    df_text = pickle.load(f)\n",
    "texts = df_text.description.tolist()\n",
    "print(\"text len:\", len(texts))\n",
    "\n",
    "n_train = 1503424\n",
    "maxlen = 100\n",
    "max_words = 10000\n",
    "\n",
    "X_text, X_test_text = texts[:n_train], texts[:n_train]\n",
    "X_text, word_index = preprocessing_valid(X_text, max_words, maxlen, n_train)\n",
    "print('done')\n",
    "# X_text['fulltrain'], X_text['test'], word_index_full = preprocessing_fulltrain(texts, max_words, maxlen, n_train)\n",
    "# print('done')\n",
    "\n",
    "# X_text['train'].shape, X_text['valid'].shape, X_text['fulltrain'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/wiki.ru.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-fd93e635ee44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0membedding_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/wiki.ru.vec'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/wiki.ru.vec'"
     ]
    }
   ],
   "source": [
    "embedding_index = {}\n",
    "f = open('../input/wiki.ru.vec', encoding='utf-8')\n",
    "for i, line in enumerate(f):\n",
    "    if i == 0: continue\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "    except ValueError:\n",
    "        pass\n",
    "f.close()\n",
    "print(\"embedding index done, len:\", len(embedding_index.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('fea0.pkl', 'rb') as file: fea0 = pickle.load(file=file)\n",
    "with open('labels.pkl', 'rb') as file: labels = pickle.load(file=file)\n",
    "category_features = fea0['cat']\n",
    "fea0.pop('cat');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_name = 'train'\n",
    "te_name = 'valid'\n",
    "\n",
    "for k in fea0.keys():\n",
    "    fea0[k] = fea0[k].drop(['user_id'], axis=1)\n",
    "category_features.remove('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\dev\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_cat = fea0[tr_name][category_features]\n",
    "test_cat = fea0[te_name][category_features]\n",
    "\n",
    "max_cat_values = []\n",
    "for c in category_features:\n",
    "    le = LabelEncoder()\n",
    "    x = le.fit_transform(pd.concat([train_cat, test_cat])[c])\n",
    "    train_cat[c] = le.transform(train_cat[c])\n",
    "    test_cat[c] = le.transform(test_cat[c])\n",
    "    max_cat_values.append(np.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = fea0[tr_name].drop(category_features, 1)\n",
    "test_num = fea0[te_name].drop(category_features, 1)\n",
    "train_num = train_num.replace([np.inf, -np.inf, np.nan], 0)\n",
    "test_num = test_num.replace([np.inf, -np.inf, np.nan], 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.vstack([train_num, test_num]))\n",
    "train_num = scaler.transform(train_num.values)\n",
    "test_num = scaler.transform(test_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = train_cat.values\n",
    "test_cat = test_cat.values\n",
    "\n",
    "x_train_cat = []\n",
    "for i in range(train_cat.shape[1]):\n",
    "    x_train_cat.append(train_cat[:, i].reshape(-1, 1))\n",
    "    \n",
    "x_test_cat = []\n",
    "for i in range(test_cat.shape[1]):\n",
    "    x_test_cat.append(test_cat[:, i].reshape(-1, 1))\n",
    "    \n",
    "train_input = x_train_cat + [train_num]\n",
    "test_input = x_test_cat + [test_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, CuDNNGRU, GlobalAveragePooling1D\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "       \n",
    "def nn_model():\n",
    "    \n",
    "    inputs = []\n",
    "    \n",
    "    # text \n",
    "    input_text = Input(shape=(maxlen,))   \n",
    "    x = Embedding(max_words, embedding_dim, input_length=maxlen, \n",
    "                  weights=[embedding_matrix], \n",
    "                  trainable=False\n",
    "                 )(input_text)\n",
    "    # convnet\n",
    "    #x = CuDNNGRU(64, return_sequences=True)(x)\n",
    "    x = Conv1D(64, 7, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(64, 7, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(64, 7, activation='relu')(x)\n",
    "    x_max = GlobalMaxPooling1D()(x)\n",
    "    x_avg = GlobalAveragePooling1D()(x)\n",
    "    flatten_t = concatenate([x_max, x_avg])\n",
    "    \n",
    "    inputs.append(input_text)    \n",
    "    \n",
    "    # cat\n",
    "    for e, c in enumerate(category_features):\n",
    "        input_c = Input(shape=(1, ), dtype='int32')\n",
    "        num_c = max_cat_values[e]\n",
    "        embed_c = Embedding(\n",
    "            num_c,\n",
    "            16,\n",
    "            input_length=1\n",
    "        )(input_c)\n",
    "        embed_c = Dropout(0.5)(embed_c)\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "        inputs.append(input_c)\n",
    "\n",
    "    # num\n",
    "    input_num = Input(shape=(train_num.shape[1],), dtype='float32')\n",
    "    inputs.append(input_num)\n",
    "    \n",
    "    # concatenate\n",
    "    flatten = concatenate([flatten_c, input_num, flatten_t])\n",
    "    \n",
    "    # dense \n",
    "    fc1 = Dense(512, kernel_initializer='he_normal')(flatten)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.75)(fc1)\n",
    "\n",
    "    fc1 = Dense(64, kernel_initializer='he_normal')(fc1)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.5)(fc1)\n",
    "\n",
    "    outputs = Dense(1, kernel_initializer='he_normal', activation=None)(fc1)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = [outputs])\n",
    "    model.compile(loss=root_mean_squared_error, optimizer='adam')\n",
    "    return (model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1103424 samples, validate on 300000 samples\n",
      "Epoch 1/15\n",
      "1103424/1103424 [==============================] - 31s 28us/step - loss: 0.3118 - val_loss: 0.2335\n",
      "Epoch 2/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2318 - val_loss: 0.2230\n",
      "Epoch 3/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2277 - val_loss: 0.2226\n",
      "Epoch 4/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2260 - val_loss: 0.2219\n",
      "Epoch 5/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2250 - val_loss: 0.2227\n",
      "Epoch 6/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2243 - val_loss: 0.2214\n",
      "Epoch 7/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2240 - val_loss: 0.2213\n",
      "Epoch 8/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2237 - val_loss: 0.2221\n",
      "Epoch 9/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2236 - val_loss: 0.2215\n",
      "Epoch 10/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2235 - val_loss: 0.2213\n",
      "Epoch 11/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2234 - val_loss: 0.2216\n",
      "Epoch 12/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2234 - val_loss: 0.2212\n",
      "Epoch 13/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2233 - val_loss: 0.2211\n",
      "Epoch 14/15\n",
      "1103424/1103424 [==============================] - 30s 27us/step - loss: 0.2233 - val_loss: 0.2211\n",
      "Epoch 15/15\n",
      "1103424/1103424 [==============================] - 29s 27us/step - loss: 0.2232 - val_loss: 0.2211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18e8c36d6a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn_model()\n",
    "model.fit(train_input, labels['train'], epochs=15, batch_size=512,\n",
    "          validation_data=(test_input, labels['valid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_67 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_64 (Embedding)        (None, 1, 16)        64          input_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 1, 16)        0           embedding_64[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_64 (Flatten)            (None, 16)           0           dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_68 (InputLayer)           (None, 51)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 67)           0           flatten_64[0][0]                 \n",
      "                                                                 input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 512)          34816       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_7 (PReLU)               (None, 512)          512         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 512)          2048        p_re_lu_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 512)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           32832       dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_8 (PReLU)               (None, 64)           64          dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64)           256         p_re_lu_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 64)           0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            65          dropout_72[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 70,657\n",
      "Trainable params: 69,505\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_seeds = 5\n",
    "# begintime = time()\n",
    "# if cv_only:\n",
    "#     for s in xrange(num_seeds):\n",
    "#         np.random.seed(s)\n",
    "#         for (inTr, inTe) in kfold.split(X, train_label):\n",
    "#             xtr = X[inTr]\n",
    "#             ytr = train_label[inTr]\n",
    "#             xte = X[inTe]\n",
    "#             yte = train_label[inTe]\n",
    "\n",
    "#             xtr_cat = X_cat[inTr]\n",
    "#             xte_cat = X_cat[inTe]\n",
    "\n",
    "#             # get xtr xte cat\n",
    "#             xtr_cat_list, xte_cat_list = [], []\n",
    "#             for i in xrange(xtr_cat.shape[1]):\n",
    "#                 xtr_cat_list.append(xtr_cat[:, i].reshape(-1, 1))\n",
    "#                 xte_cat_list.append(xte_cat[:, i].reshape(-1, 1))\n",
    "\n",
    "#             xtr_cat_list.append(xtr)\n",
    "#             xte_cat_list.append(xte)\n",
    "\n",
    "#             model = nn_model()\n",
    "#             def get_rank(x):\n",
    "#                 return pd.Series(x).rank(pct=True).values\n",
    "#             model.fit(xtr_cat_list, ytr, epochs=20, batch_size=512, verbose=2, validation_data=[xte_cat_list, yte])\n",
    "#             cv_train[inTe] += get_rank(model.predict(x=xte_cat_list, batch_size=512, verbose=0)[:, 0])\n",
    "#             print(Gini(train_label[inTe], cv_train[inTe]))\n",
    "#             cv_pred += get_rank(model.predict(x=x_test_cat, batch_size=512, verbose=0)[:, 0])\n",
    "#         print(s)\n",
    "#         print(Gini(train_label, cv_train / (1. * (s + 1))))\n",
    "#         print(str(datetime.timedelta(seconds=time() - begintime)))\n",
    "#     if save_cv:\n",
    "#         pd.DataFrame({'id': test_id, 'target': get_rank(cv_pred * 1./ (NFOLDS * num_seeds))}).to_csv('../model/keras5_pred.csv', index=False)\n",
    "#         pd.DataFrame({'id': train_id, 'target': get_rank(cv_train * 1. / num_seeds)}).to_csv('../model/keras5_cv.csv', index=False)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
