{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import gc \n",
    "\n",
    "from avito_functions import * \n",
    "from avito_classes import TargetEncoder\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix, vstack\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import compress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load df\n",
      "Load agg input\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "\n",
    "data_keys = ['train', 'valid', 'holdout', 'fulltrain', 'test']\n",
    "\n",
    "print('Load df')\n",
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "print('Load agg input')\n",
    "with open('../input/map_dict.pkl', 'rb') as file: map_dict = pickle.load(file)\n",
    "#with open('../input/text_features.pkl', 'rb') as f: X_text = pickle.load(f)\n",
    "with open('../input/text_num_features.pkl', 'rb') as f: X_text_num = pickle.load(f)\n",
    "sgd = load_fe('sgd2')\n",
    "extra = load_fe('extra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run preprocessing..\n",
      "run feature engineering..\n",
      "-- count fraction price_x_region__category_name_frac\n",
      "-- count fraction price_x_region__param_1_frac\n",
      "-- count fraction price_x_region__param_2_frac\n",
      "-- count fraction price_x_region__image_top_1_frac\n",
      "-- count fraction price_x_city__category_name_frac\n",
      "-- count fraction price_x_city__param_1_frac\n",
      "-- count fraction price_x_city__param_2_frac\n",
      "-- count fraction price_x_city__image_top_1_frac\n",
      "-- count fraction price_x_image_top_1__category_name_frac\n",
      "-- count fraction price_x_image_top_1__param_1_frac\n",
      "-- count fraction price_x_image_top_1__param_2_frac\n",
      "-- count fraction price_x_population_groups__param_1_frac\n",
      "-- combine factors: price_log_cut_x_parent_category_name\n",
      "-- combine factors: price_log_cut_x_category_name\n",
      "-- combine factors: price_log_cut_x_region\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2018-06-26 18:13:30.559660'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pipeline\n",
    "n_train = df_train.shape[0]\n",
    "add_features = X_text_num\n",
    "\n",
    "X, y, category_features = preprocessing(df_train, df_test, map_dict, add_features)\n",
    "X, category_features = feature_engineering(X, category_features)\n",
    "\n",
    "for f in category_features:\n",
    "    X[f] = pd.factorize(X[f])[0]\n",
    "    \n",
    "del df_train, df_test\n",
    "gc.collect()\n",
    "str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "\n",
    "del X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run validation splitting..\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, x_holdout, \\\n",
    "y_train, y_valid, y_holdout, \\\n",
    "_,_,_ = validation_split(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, k in zip([x_train, x_valid, x_holdout, X_train, X_test], data_keys):\n",
    "    x['sgd'] = sgd[k]\n",
    "    x['ext'] = extra[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- target encoding: ['region']\n",
      "-- target encoding: ['region']\n",
      "-- target encoding: ['city']\n",
      "-- target encoding: ['city']\n",
      "-- target encoding: ['parent_category_name']\n",
      "-- target encoding: ['parent_category_name']\n",
      "-- target encoding: ['category_name']\n",
      "-- target encoding: ['category_name']\n",
      "-- target encoding: ['param_1']\n",
      "-- target encoding: ['param_1']\n",
      "-- target encoding: ['param_2']\n",
      "-- target encoding: ['param_2']\n",
      "-- target encoding: ['param_3']\n",
      "-- target encoding: ['param_3']\n",
      "-- target encoding: ['user_type']\n",
      "-- target encoding: ['user_type']\n",
      "-- target encoding: ['image_top_1']\n",
      "-- target encoding: ['image_top_1']\n",
      "-- target encoding: ['price_log_cut_x_parent_category_name']\n",
      "-- target encoding: ['price_log_cut_x_parent_category_name']\n",
      "-- target encoding: ['price_log_cut_x_category_name']\n",
      "-- target encoding: ['price_log_cut_x_category_name']\n",
      "-- target encoding: ['price_log_cut_x_region']\n",
      "-- target encoding: ['price_log_cut_x_region']\n",
      "-- target encoding: ['price_exists']\n",
      "-- target encoding: ['price_exists']\n",
      "-- target encoding: ['image_exists']\n",
      "-- target encoding: ['image_exists']\n",
      "-- target encoding: ['descr_exists']\n",
      "-- target encoding: ['descr_exists']\n",
      "-- target encoding: ['weekday']\n",
      "-- target encoding: ['weekday']\n",
      "-- target encoding: ['free_day']\n",
      "-- target encoding: ['free_day']\n",
      "-- target encoding: ['population_groups']\n",
      "-- target encoding: ['population_groups']\n",
      "-- target encoding: ['price_log_cut', 'category_name']\n",
      "-- target encoding: ['price_log_cut', 'category_name']\n",
      "-- target encoding: ['price_log_cut', 'region']\n",
      "-- target encoding: ['price_log_cut', 'region']\n",
      "-- target encoding: ['price_log_cut', 'param_1']\n",
      "-- target encoding: ['price_log_cut', 'param_1']\n",
      "-- target encoding: ['region', 'parent_category_name']\n",
      "-- target encoding: ['region', 'parent_category_name']\n"
     ]
    }
   ],
   "source": [
    "# target encoding \n",
    "\n",
    "te_groups = []\n",
    "for f in category_features:\n",
    "    te_groups.append([f])\n",
    "\n",
    "te_groups += [['price_log_cut', 'category_name'], \n",
    "              ['price_log_cut', 'region'],\n",
    "              ['price_log_cut', 'param_1'],\n",
    "              ['region', 'parent_category_name']\n",
    "             ]\n",
    "\n",
    "for group in te_groups:\n",
    "    x_train, x_valid, x_holdout = target_encoding(x_train, y_train, x_valid, group, x_holdout)\n",
    "    X_train, X_test = target_encoding(X_train, y, X_test, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103424, 57) True\n",
      "(300000, 57) True\n",
      "(100000, 57) True\n",
      "(1503424, 57) True\n",
      "(508438, 57) True\n"
     ]
    }
   ],
   "source": [
    "cat_data = []\n",
    "for x in [x_train, x_valid, x_holdout, X_train, X_test]:\n",
    "    cat_data.append(x[category_features])\n",
    "    x.drop(category_features, 1, inplace=True)\n",
    "    print(x.shape, all(x.columns == x_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impute numeric\n",
      "scale numeric\n"
     ]
    }
   ],
   "source": [
    "## impute \n",
    "print('impute numeric')\n",
    "x_train, x_valid, x_holdout, _ = num_fillna(x_train, x_valid, x_holdout)\n",
    "X_train, X_test, _ = num_fillna(X_train, X_test)\n",
    "\n",
    "## scale\n",
    "print('scale numeric')\n",
    "x_train, x_valid, x_holdout, _ = num_scaling(x_train, x_valid, x_holdout)\n",
    "X_train, X_test, _ = num_scaling(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = [y_train, y_valid, y_holdout]\n",
    "for x in [x_train, x_valid, x_holdout, X_train, X_test]:\n",
    "    data.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HDBSCAN(algorithm='best', allow_single_cluster=False, alpha=1.0,\n",
       "    approx_min_span_tree=True, cluster_selection_method='eom',\n",
       "    core_dist_n_jobs=4, gen_min_span_tree=False, leaf_size=40,\n",
       "    match_reference_implementation=False, memory=Memory(cachedir=None),\n",
       "    metric='euclidean', min_cluster_size=100, min_samples=None, p=None,\n",
       "    prediction_data=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=100)\n",
    "clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.fit(X_train.append(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# params = {'n_neighbors': 10, 'n_jobs': 3}\n",
    "# pred_val, pred_hol, extra = train_sklearn(KNeighborsRegressor, params, data, labels)\n",
    "# str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # kmeans\n",
    "# from sklearn.cluster import KMeans\n",
    "# for k in [30, 50]:\n",
    "#     kmeans = KMeans(k, n_jobs=3)\n",
    "#     kmeans_labels = kmeans.fit_predict(X_train.append(X_test))\n",
    "#     d_preds = {}\n",
    "#     d_preds['fulltrain'] = kmeans_labels[:n_train]\n",
    "#     d_preds['test'] = kmeans_labels[n_train:]\n",
    "#     _,_,_, d_preds['train'],d_preds['valid'],d_preds['holdout'] ,_,_,_ = validation_split(X_train, kmeans_labels)\n",
    "#     with open('../fe/kmeans{}.pkl'.format(str(k)), 'wb') as file: pickle.dump(file=file, obj=d_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "# bandwidth = estimate_bandwidth(X_train.append(X_test), quantile=0.2, n_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms = MeanShift(bandwidth=bandwidth, bin_seeding=True, cluster_all=False, n_jobs=2)\n",
    "# ms.fit(X_train.append(X_test))\n",
    "# labels = ms.labels_\n",
    "# cluster_centers = ms.cluster_centers_\n",
    "# labels_unique = np.unique(labels)\n",
    "# n_clusters_ = len(labels_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = ExtraTreesRegressor(**params)\n",
    "\n",
    "# # valid \n",
    "# data = [x_train.values, x_valid.values, x_holdout.values]\n",
    "# preds = oof_prediction(model, data, y_train)\n",
    "# # test\n",
    "# data = [X_train.values, X_test.values]\n",
    "# preds += oof_prediction(model, data, y)\n",
    "\n",
    "# d_preds = {}\n",
    "# for pred, k in zip(preds, ['train', 'valid', 'holdout', 'fulltrain', 'test']):\n",
    "#     d_preds[k] = pred\n",
    "    \n",
    "# with open('../fe/knn.pkl', 'wb') as file: pickle.dump(file=file, obj=d_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
