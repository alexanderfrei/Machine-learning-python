{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import re\n",
    "import gc \n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from avito_functions import preprocessing\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import time \n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_repl = {\"эт.\": \"этаж\", \n",
    "             \"сот.\": \"сотка\",\n",
    "             \"кг.\": \"килограмм\",\n",
    "             \"р.\": \"размер\",\n",
    "             \"арт.\": \"артикул\",\n",
    "             \"art.\": \"артикул\",\n",
    "             \"нат.\": \"натуральная\",\n",
    "             \"натур.\": \"натуральная\",\n",
    "             \"г.\": \"город\",\n",
    "             \"лада\": \"lada\",\n",
    "            }\n",
    "\n",
    "def clean(d):\n",
    "\n",
    "    d = str(d).lower()\n",
    "    d = d.replace('nan', '')\n",
    "    # автозамена сокращений \n",
    "    for a, r in abbr_repl.items():\n",
    "        d = d.replace(a, ' ' + r + ' ')\n",
    "    # дроби\n",
    "    d = re.sub(r\"([0-9])[^0-9]([0-9])\", r\"\\1x\\2\", d)\n",
    "    # дюймы \n",
    "    d = re.sub(r\"([0-9])[\\'\\\"\\*]\", r\"\\1in\", d)\n",
    "    # удаляем пунктуацию кроме -\n",
    "    d = re.sub(r\"[^\\w\\s\\-]\", r\" \", d)\n",
    "    # удаляем - в начале и конце слов\n",
    "    d = re.sub(r\"(\\s)[\\.-](\\w)\", r\"\\1\\2\", d)\n",
    "    d = re.sub(r\"(\\w)[\\.-](\\s)\", r\"\\1\\2\", d)\n",
    "    # удаляем несколько пробелов подряд \n",
    "    d = re.sub(r\"( )+\", r\" \", d).strip()\n",
    "    \n",
    "    # лемматизация \n",
    "    d = ' '.join([ma.parse(w)[0].normal_form for w in d.split()])\n",
    "    return d\n",
    "\n",
    "ma = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input \n",
    "\n",
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../desc.txt', 'w', encoding='utf-8') as f:\n",
    "#     for t in df_train.description.head(50000):\n",
    "#         f.write(str(t).replace('\\n', ' ')+'\\n')\n",
    "\n",
    "# with open('../params.txt', 'w', encoding='utf-8') as f:\n",
    "#     for _, p1, p2, p3 in df_train[['param_1', 'param_2', 'param_3']].head(50000).itertuples():\n",
    "#         f.write(' -- '.join(map(str, [p1,p2,p3])) + '\\n' )\n",
    "        \n",
    "# with open('../title.txt', 'w', encoding='utf-8') as f:\n",
    "#     for t in df_train.title.head(50000).apply(clean):\n",
    "#         f.write(str(t)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = df_train.shape[0]\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df.index = np.arange(df.shape[0])\n",
    "\n",
    "del df_train, df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preproc param..\n",
      "description\n",
      "text_feat\n",
      "title\n"
     ]
    }
   ],
   "source": [
    "# param\n",
    "\n",
    "print('preproc param..')\n",
    "df['text_feat'] = df.apply(lambda row: ' '.join([\n",
    "    str(row['param_1']), \n",
    "    str(row['param_2']), \n",
    "    str(row['param_3'])]),axis=1) # Group Param Features\n",
    "df.drop([\"param_1\",\"param_2\",\"param_3\"],axis=1,inplace=True)\n",
    "\n",
    "textfeats = [\"description\", \"text_feat\", \"title\"]\n",
    "df = df[textfeats]\n",
    "\n",
    "for cols in textfeats:\n",
    "    print(cols)\n",
    "    df[cols] = df[cols].astype(str)\n",
    "    df[cols] = df[cols].apply(clean)\n",
    "    df[cols + '_num_chars'] = df[cols].apply(len)\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split()))\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100\n",
    "    \n",
    "# with open('../input/text_num_features_clean.pkl', 'wb') as f: pickle.dump(obj=df.iloc[:, 3:], file=f)\n",
    "# with open('../input/text_features_clean.pkl', 'wb') as f: pickle.dump(obj=df.iloc[:, :3], file=f)\n",
    "    \n",
    "with open('../input/text_num_features_lemm.pkl', 'wb') as f: pickle.dump(obj=df.iloc[:, 3:], file=f)\n",
    "with open('../input/text_features_lemm.pkl', 'wb') as f: pickle.dump(obj=df.iloc[:, :3], file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TSVD tfidf 1\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def train_tsvd(n, tfidf_dict):\n",
    "    print('-- tSVD:', n)\n",
    "    ret = {}\n",
    "    tsvd = TruncatedSVD(n_components=n, random_state=2018)\n",
    "    ret['train'] = tsvd.fit_transform(tfidf_dict['train'])\n",
    "    ret['valid'] = tsvd.transform(tfidf_dict['valid'])\n",
    "    ret['holdout'] = tsvd.transform(tfidf_dict['holdout'])    \n",
    "    ret['fulltrain'] = tsvd.fit_transform(tfidf_dict['fulltrain'])\n",
    "    ret['test'] = tsvd.transform(tfidf_dict['test'])\n",
    "    with open('../fe/tfidf_svd' + str(n) + '.pkl', 'wb') as file: pickle.dump(file=file, obj=ret)\n",
    "    return ret\n",
    "\n",
    "with open('../input/tfidf_1.pkl', 'rb') as f: \n",
    "    tfidf_dict = pickle.load(f)\n",
    "\n",
    "n = 20\n",
    "fe_tfidf_svd = train_tsvd(n, tfidf_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
