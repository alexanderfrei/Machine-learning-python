{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score, roc_curve, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, Imputer, LabelBinarizer\n",
    "import category_encoders\n",
    "\n",
    "import time \n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ginic(actual, pred):\n",
    "    actual = np.asarray(actual)\n",
    "    n = len(actual)\n",
    "    a_s = actual[np.argsort(pred)]\n",
    "    a_c = a_s.cumsum()\n",
    "    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2\n",
    "    return 2 * giniSum / n\n",
    "\n",
    "def gini_normalizedc(a, p):\n",
    "    if p.ndim == 2:  # Required for sklearn wrapper\n",
    "        p = p[:, 1]  # If proba array contains proba for both 0 and 1 classes, just pick class 1\n",
    "    return ginic(a, p) / ginic(a, a)\n",
    "\n",
    "\n",
    "def cross_val(X, y, model, kf):\n",
    "    X, y = np.array(X), np.array(y).reshape(-1)\n",
    "    cv_scores = np.zeros((5,4), dtype=np.float32)\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "\n",
    "        print( \"Fold \", i)\n",
    "\n",
    "        y_train, y_val = y[train_index].copy(), y[val_index].copy()\n",
    "        X_train, X_val = X[train_index, :].copy(), X[val_index, :].copy()\n",
    "        \n",
    "        fit_model = model.fit(X_train, y_train)\n",
    "        pred = fit_model.predict(X_val)\n",
    "\n",
    "        cv_scores[i, :] = [f1_score(y_val, pred), \n",
    "                           precision_score(y_val, pred), \n",
    "                           recall_score(y_val, pred), \n",
    "                           roc_auc_score(y_val, pred)]\n",
    "        \n",
    "    return cv_scores\n",
    "\n",
    "def print_metrics(cv_scores):\n",
    "    metrics = ['f1', 'precision', 'recall', 'roc auc']\n",
    "    cvmean = cv_scores.mean(0)\n",
    "    for i in range(4):\n",
    "        print(\"{} = {:.5f}\".format(metrics[i], cvmean[i]))\n",
    "\n",
    "def cutoff_metrics(cutoff, predict_proba):\n",
    "    for cut in cutoff:\n",
    "        print(\"f1: {:.3}\\trecall: {:.3}\\tprecision: {:.3}\\tauc: {:.3}\\tacc: {:.3}\\tcutoff: {:.2} \".format(\n",
    "            f1_score(y_valid, predict_proba > cut), \n",
    "            recall_score(y_valid, predict_proba > cut), \n",
    "            precision_score(y_valid, predict_proba > cut), \n",
    "            roc_auc_score(y_valid, predict_proba > cut),\n",
    "            accuracy_score(y_valid, predict_proba>cut),\n",
    "            cut\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(train, test, target, to_drop=False, \n",
    "              high_cardinality=\"smoothing\", hc_treshold = 10, hc_drop=False, # high cardinality categorical\n",
    "              eb_k=50, eb_f=10,  # parameters for hc smoothing function \n",
    "              encode=False,  # categorical \n",
    "              fill_num=-1, scaling=False  # continuous \n",
    "             ):\n",
    "    \n",
    "    \"\"\" \n",
    "    data preprocessing \n",
    "    \n",
    "    :train, test: pandas DataFrame\n",
    "    :high_cardinality: way to handle categorical features with high number of levels\n",
    "    :encode: category encoding, 'ohe' = one hot, 'bin' = binary\n",
    "    :fill_num: fill nan for continuous features, -1 = with -1, ('mean', 'median') = strategy\n",
    "    :scaling: 'standard' = StandartScaler\n",
    "    \n",
    "    category features should have type 'object'\n",
    "    \"\"\"\n",
    "\n",
    "    # remove duplicates \n",
    "    if to_drop:\n",
    "        train = train.drop(to_drop, axis=1)\n",
    "        test = test.drop(to_drop, axis=1)\n",
    "    \n",
    "    ######## categorical features \n",
    "    \n",
    "    cat_features = train.columns[train.dtypes=='object']\n",
    "    num_features = train.columns[train.dtypes!='object']      \n",
    "        \n",
    "    # factorize \n",
    "    le = LabelEncoder()\n",
    "    train[cat_features] = train[cat_features].fillna('-1')\n",
    "    test[cat_features] = test[cat_features].fillna('-1')\n",
    "    for c in cat_features:\n",
    "        data=train[c].append(test[c])\n",
    "        le.fit(data.values.tolist())  # nan = 0 level\n",
    "        train[c] = le.transform(train[c].values.tolist())\n",
    "        test[c] = le.transform(test[c].values.tolist())       \n",
    "    \n",
    "    # mark nan with -1, if encoding not necessary \n",
    "    if not encode:\n",
    "        train[cat_features] = train[cat_features].replace(0, -1)\n",
    "        test[cat_features] = test[cat_features].replace(0, -1)        \n",
    "    ######## high cardinality\n",
    "    \n",
    "    if high_cardinality:\n",
    "\n",
    "        hc_features = train[cat_features].columns[train[cat_features].apply(lambda x: len(x.value_counts())) > hc_treshold]\n",
    "        target_mean = target.mean()\n",
    "        S = {}\n",
    "\n",
    "        for c in hc_features:\n",
    "\n",
    "            if high_cardinality == \"sr\":\n",
    "                # supervised ratio \n",
    "                group_means = pd.concat([train[c], pd.DataFrame(target, columns=['target'], index=train.index)], axis=1).groupby(c).mean()\n",
    "                group_means = group_means.target.to_dict()\n",
    "                for group in train[c].value_counts().index:\n",
    "                    S[group] = group_means[group]\n",
    "\n",
    "            if high_cardinality==\"woe\":\n",
    "                # weight of evidence\n",
    "                group_y1 = pd.concat([train[c], pd.DataFrame(target, columns=['target'], index=train.index)], axis=1).\\\n",
    "                groupby([c]).agg('sum')\n",
    "                group_y0 = pd.concat([train[c], pd.DataFrame(target, columns=['target'], index=train.index)], axis=1).\\\n",
    "                groupby([c]).agg('count') - group_y1\n",
    "                y1 = (target==1).sum()\n",
    "                y0 = (target==0).sum()\n",
    "                woe = np.log(((group_y1) / y1) / ((group_y0) / y0))\n",
    "                for i,v in zip(woe.index, np.where(np.isinf(woe), 0, woe)):\n",
    "                    S[i] = v[0]\n",
    "\n",
    "            if high_cardinality==\"smoothing\":\n",
    "                # empirical bayes (smoothing for small group)\n",
    "                group_means = pd.concat([train[c], pd.DataFrame(target, columns=['target'], index=train.index)], axis=1).groupby(c).mean()\n",
    "                group_means = group_means.target.to_dict()\n",
    "                group_counts = pd.concat([train[c], pd.DataFrame(target, columns=['target'], index=train.index)], axis=1).groupby(c).agg('count')\n",
    "                group_counts = group_counts.target.to_dict()\n",
    "\n",
    "                def smoothing_function(n, k, f):\n",
    "                    return 1 / (1 + np.exp(-(n-k)/f))\n",
    "\n",
    "                for group in train[c].value_counts().index:\n",
    "                    lam = smoothing_function(n=group_counts[group], k=eb_k, f=eb_f)\n",
    "                    S[group] = lam*group_means[group] + (1-lam)*target_mean\n",
    "\n",
    "            # transform train\n",
    "            train[c+'_avg'] = train[c].apply(lambda x: S[x]).copy()\n",
    "\n",
    "            # transform test\n",
    "            def hc_transform_test(x):\n",
    "                if x in S: \n",
    "                    return S[x]\n",
    "                else:\n",
    "                    return target_mean\n",
    "\n",
    "            test[c+'_avg'] = test[c].apply(hc_transform_test).copy()\n",
    "\n",
    "        # drop hc features \n",
    "        if hc_drop:\n",
    "            train.drop(hc_features, axis=1, inplace=True)\n",
    "            test.drop(hc_features, axis=1, inplace=True)\n",
    "\n",
    "        # update cat features \n",
    "        cat_features = sorted(list(set(cat_features).difference(hc_features)))\n",
    "\n",
    "    ######## for linear models \n",
    "    \n",
    "    # fill missings\n",
    "    if fill_num in ['mean', 'median']:\n",
    "        imputer = Imputer(strategy=fill_num)\n",
    "        train[num_features] = imputer.fit_transform(train[num_features])\n",
    "        test[num_features] = imputer.transform(test[num_features])\n",
    "    elif fill_num < 0:\n",
    "        train[num_features] = train[num_features].fillna(fill_num)\n",
    "        test[num_features] = test[num_features].fillna(fill_num)\n",
    "        \n",
    "    # scaling\n",
    "    if scaling=='standard':\n",
    "        scaler = StandardScaler()\n",
    "        train[num_features] = scaler.fit_transform(train[num_features])\n",
    "        test[num_features] = scaler.transform(test[num_features])\n",
    "    \n",
    "    ######## encoding \n",
    "    if encode=='ohe':\n",
    "        # one hot encoding, memory inefficient\n",
    "        oh = OneHotEncoder(sparse=False)\n",
    "        for c in cat_features:\n",
    "            data=train[c].append(test[c])\n",
    "            oh.fit(data.reshape(-1,1))            \n",
    "            train_temp = oh.transform(train[c].reshape(-1,1))\n",
    "            test_temp = oh.transform(test[c].reshape(-1,1))\n",
    "            train = pd.concat([train, pd.DataFrame(train_temp, \n",
    "                                                   columns=[(c+\"_\"+str(i)) for i in data.value_counts().index],\n",
    "                                                   index = train.index\n",
    "                                                  )], axis=1)\n",
    "            test = pd.concat([test, pd.DataFrame(test_temp, \n",
    "                                                 columns=[(c+\"_\"+str(i)) for i in data.value_counts().index],\n",
    "                                                 index = test.index\n",
    "                                                )], axis=1)\n",
    "            # drop column\n",
    "            train.drop(c, axis=1, inplace=True)\n",
    "            test.drop(c, axis=1, inplace=True)\n",
    "    \n",
    "    if encode=='bin':\n",
    "        # binary encoding \n",
    "        pass\n",
    "            \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = transform(train, test, target, \n",
    "#                         encode='ohe', scaling=True, fill_num='median', hc_drop=True,\n",
    "#                         to_drop=['Var214', 'Var220', 'Var222'])\n",
    "# train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.isnull().sum().value_counts()[:5]\n",
    "# nan_group_1 = X[X.columns[X.isnull().sum() == 34506]].dropna().index\n",
    "# nan_group_2 = X[X.columns[X.isnull().sum() == 34141]].dropna().index\n",
    "# nan_group_3 = X[X.columns[X.isnull().sum() == 33975]].dropna().index\n",
    "# nan_group_4 = X[X.columns[X.isnull().sum() == 33881]].dropna().index\n",
    "# set(nan_group_3).intersection(list(nan_group_2))\n",
    "# выставим порог отсева пустых значений на 99%\n",
    "# X = X[X.columns[(X.isnull().sum() / X.shape[0] ) < 0.75]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 77) (10000, 77)\n",
      "Fold  0\n",
      "Fold  1\n",
      "Fold  2\n",
      "Fold  3\n",
      "Fold  4\n",
      "f1 = 0.08469\n",
      "precision = 0.59812\n",
      "recall = 0.04569\n",
      "roc auc = 0.52160\n"
     ]
    }
   ],
   "source": [
    "MISS_THERSHOLD = 0.95\n",
    "ENCODE = False\n",
    "FILL_NUM = -1\n",
    "\n",
    "# high cardinality \n",
    "HC = \"smoothing\"\n",
    "HC_DROP = False\n",
    "HC_K = 50\n",
    "HC_F = 2\n",
    "\n",
    "############################################################################################################\n",
    "# xgb hyperparameters\n",
    "\n",
    "# model \n",
    "model = XGBClassifier(n_jobs=4, tree_method='gpu_hist', predictor = \"cpu_predictor\", objective=\"binary:logistic\",\n",
    "                      n_estimators=200, \n",
    "                      learning_rate=0.05,\n",
    "                      max_depth=5,\n",
    "                      gamma=10, min_child_weight=2,\n",
    "                      subsample=.8, colsample_bytree=.8,\n",
    "#                       scale_pos_weight=1.5,\n",
    "                      reg_alpha=2,\n",
    "                      reg_lambda=1.3,\n",
    "                     )\n",
    "\n",
    "############################################################################################################\n",
    "# load \n",
    "test = pd.read_csv('./input/orange_small_churn_test_data.csv')\n",
    "train, target = pd.read_csv('./input/orange_small_churn_data.train'), \\\n",
    "np.where(pd.read_csv('./input/orange_small_churn_labels.train', header=-1)==1, 1, 0).ravel()\n",
    "\n",
    "test_id= test['ID']\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "# выставим порог отсева пустых значений на 95%\n",
    "to_drop = train.columns[(train.isnull().sum() / train.shape[0] ) >= MISS_THERSHOLD]\n",
    "train.drop(to_drop, axis=1, inplace=True)\n",
    "test.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# split data \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=5000, random_state=42, stratify=target)\n",
    "\n",
    "# transform\n",
    "X_train, X_valid = transform(X_train, X_valid, y_train, \n",
    "                        encode=ENCODE, scaling=True, fill_num=FILL_NUM, \n",
    "                        hc_drop=HC_DROP, high_cardinality=HC, eb_k=HC_K, eb_f=HC_F,\n",
    "                        to_drop=['Var214', 'Var220', 'Var222'])\n",
    "print(train.shape, test.shape )\n",
    "\n",
    "# cv \n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "cv_scores = cross_val(X_train, y_train, model, kf)\n",
    "print_metrics(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "model = XGBClassifier(n_jobs=4, tree_method='gpu_hist', predictor = \"cpu_predictor\", objective=\"binary:logistic\",\n",
    "                      n_estimators=211, \n",
    "                      learning_rate=0.05,\n",
    "                      max_depth=5,\n",
    "                      gamma=5, min_child_weight=2,\n",
    "                      subsample=.8, colsample_bytree=.8,\n",
    "#                       scale_pos_weight=1.5,\n",
    "                      reg_alpha=2,\n",
    "                      reg_lambda=1.3,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.655619\n",
      "Will train until validation_0-auc hasn't improved in 20 rounds.\n",
      "[1]\tvalidation_0-auc:0.666381\n",
      "[2]\tvalidation_0-auc:0.666151\n",
      "[3]\tvalidation_0-auc:0.671602\n",
      "[4]\tvalidation_0-auc:0.672713\n",
      "[5]\tvalidation_0-auc:0.676607\n",
      "[6]\tvalidation_0-auc:0.679461\n",
      "[7]\tvalidation_0-auc:0.687289\n",
      "[8]\tvalidation_0-auc:0.688947\n",
      "[9]\tvalidation_0-auc:0.687634\n",
      "[10]\tvalidation_0-auc:0.688155\n",
      "[11]\tvalidation_0-auc:0.689897\n",
      "[12]\tvalidation_0-auc:0.689954\n",
      "[13]\tvalidation_0-auc:0.68649\n",
      "[14]\tvalidation_0-auc:0.683896\n",
      "[15]\tvalidation_0-auc:0.684376\n",
      "[16]\tvalidation_0-auc:0.686283\n",
      "[17]\tvalidation_0-auc:0.687387\n",
      "[18]\tvalidation_0-auc:0.689326\n",
      "[19]\tvalidation_0-auc:0.690723\n",
      "[20]\tvalidation_0-auc:0.68946\n",
      "[21]\tvalidation_0-auc:0.685064\n",
      "[22]\tvalidation_0-auc:0.685426\n",
      "[23]\tvalidation_0-auc:0.685907\n",
      "[24]\tvalidation_0-auc:0.686403\n",
      "[25]\tvalidation_0-auc:0.687594\n",
      "[26]\tvalidation_0-auc:0.686322\n",
      "[27]\tvalidation_0-auc:0.687625\n",
      "[28]\tvalidation_0-auc:0.686105\n",
      "[29]\tvalidation_0-auc:0.688048\n",
      "[30]\tvalidation_0-auc:0.690495\n",
      "[31]\tvalidation_0-auc:0.690646\n",
      "[32]\tvalidation_0-auc:0.691788\n",
      "[33]\tvalidation_0-auc:0.693215\n",
      "[34]\tvalidation_0-auc:0.691136\n",
      "[35]\tvalidation_0-auc:0.693532\n",
      "[36]\tvalidation_0-auc:0.692105\n",
      "[37]\tvalidation_0-auc:0.692795\n",
      "[38]\tvalidation_0-auc:0.693403\n",
      "[39]\tvalidation_0-auc:0.693982\n",
      "[40]\tvalidation_0-auc:0.695149\n",
      "[41]\tvalidation_0-auc:0.696702\n",
      "[42]\tvalidation_0-auc:0.695867\n",
      "[43]\tvalidation_0-auc:0.695953\n",
      "[44]\tvalidation_0-auc:0.695575\n",
      "[45]\tvalidation_0-auc:0.694317\n",
      "[46]\tvalidation_0-auc:0.695167\n",
      "[47]\tvalidation_0-auc:0.697443\n",
      "[48]\tvalidation_0-auc:0.697338\n",
      "[49]\tvalidation_0-auc:0.698242\n",
      "[50]\tvalidation_0-auc:0.699428\n",
      "[51]\tvalidation_0-auc:0.701189\n",
      "[52]\tvalidation_0-auc:0.702829\n",
      "[53]\tvalidation_0-auc:0.703694\n",
      "[54]\tvalidation_0-auc:0.703009\n",
      "[55]\tvalidation_0-auc:0.703872\n",
      "[56]\tvalidation_0-auc:0.704914\n",
      "[57]\tvalidation_0-auc:0.705589\n",
      "[58]\tvalidation_0-auc:0.706207\n",
      "[59]\tvalidation_0-auc:0.706579\n",
      "[60]\tvalidation_0-auc:0.706523\n",
      "[61]\tvalidation_0-auc:0.706793\n",
      "[62]\tvalidation_0-auc:0.706899\n",
      "[63]\tvalidation_0-auc:0.707327\n",
      "[64]\tvalidation_0-auc:0.707538\n",
      "[65]\tvalidation_0-auc:0.707367\n",
      "[66]\tvalidation_0-auc:0.707642\n",
      "[67]\tvalidation_0-auc:0.706883\n",
      "[68]\tvalidation_0-auc:0.707854\n",
      "[69]\tvalidation_0-auc:0.707117\n",
      "[70]\tvalidation_0-auc:0.706855\n",
      "[71]\tvalidation_0-auc:0.707308\n",
      "[72]\tvalidation_0-auc:0.708292\n",
      "[73]\tvalidation_0-auc:0.708139\n",
      "[74]\tvalidation_0-auc:0.708922\n",
      "[75]\tvalidation_0-auc:0.709805\n",
      "[76]\tvalidation_0-auc:0.709207\n",
      "[77]\tvalidation_0-auc:0.709102\n",
      "[78]\tvalidation_0-auc:0.709635\n",
      "[79]\tvalidation_0-auc:0.710261\n",
      "[80]\tvalidation_0-auc:0.710683\n",
      "[81]\tvalidation_0-auc:0.710029\n",
      "[82]\tvalidation_0-auc:0.710223\n",
      "[83]\tvalidation_0-auc:0.710241\n",
      "[84]\tvalidation_0-auc:0.709862\n",
      "[85]\tvalidation_0-auc:0.710156\n",
      "[86]\tvalidation_0-auc:0.710435\n",
      "[87]\tvalidation_0-auc:0.710211\n",
      "[88]\tvalidation_0-auc:0.71048\n",
      "[89]\tvalidation_0-auc:0.710385\n",
      "[90]\tvalidation_0-auc:0.709715\n",
      "[91]\tvalidation_0-auc:0.71032\n",
      "[92]\tvalidation_0-auc:0.710405\n",
      "[93]\tvalidation_0-auc:0.710763\n",
      "[94]\tvalidation_0-auc:0.710918\n",
      "[95]\tvalidation_0-auc:0.711411\n",
      "[96]\tvalidation_0-auc:0.711465\n",
      "[97]\tvalidation_0-auc:0.710995\n",
      "[98]\tvalidation_0-auc:0.711569\n",
      "[99]\tvalidation_0-auc:0.711638\n",
      "[100]\tvalidation_0-auc:0.711671\n",
      "[101]\tvalidation_0-auc:0.711693\n",
      "[102]\tvalidation_0-auc:0.711905\n",
      "[103]\tvalidation_0-auc:0.712197\n",
      "[104]\tvalidation_0-auc:0.711791\n",
      "[105]\tvalidation_0-auc:0.7115\n",
      "[106]\tvalidation_0-auc:0.711123\n",
      "[107]\tvalidation_0-auc:0.711615\n",
      "[108]\tvalidation_0-auc:0.711742\n",
      "[109]\tvalidation_0-auc:0.712043\n",
      "[110]\tvalidation_0-auc:0.711926\n",
      "[111]\tvalidation_0-auc:0.712057\n",
      "[112]\tvalidation_0-auc:0.71224\n",
      "[113]\tvalidation_0-auc:0.712536\n",
      "[114]\tvalidation_0-auc:0.712788\n",
      "[115]\tvalidation_0-auc:0.712704\n",
      "[116]\tvalidation_0-auc:0.712494\n",
      "[117]\tvalidation_0-auc:0.712168\n",
      "[118]\tvalidation_0-auc:0.712208\n",
      "[119]\tvalidation_0-auc:0.711852\n",
      "[120]\tvalidation_0-auc:0.712102\n",
      "[121]\tvalidation_0-auc:0.712101\n",
      "[122]\tvalidation_0-auc:0.712273\n",
      "[123]\tvalidation_0-auc:0.712305\n",
      "[124]\tvalidation_0-auc:0.712348\n",
      "[125]\tvalidation_0-auc:0.712169\n",
      "[126]\tvalidation_0-auc:0.712546\n",
      "[127]\tvalidation_0-auc:0.712551\n",
      "[128]\tvalidation_0-auc:0.712699\n",
      "[129]\tvalidation_0-auc:0.712716\n",
      "[130]\tvalidation_0-auc:0.713208\n",
      "[131]\tvalidation_0-auc:0.713163\n",
      "[132]\tvalidation_0-auc:0.713254\n",
      "[133]\tvalidation_0-auc:0.713449\n",
      "[134]\tvalidation_0-auc:0.713401\n",
      "[135]\tvalidation_0-auc:0.71351\n",
      "[136]\tvalidation_0-auc:0.713681\n",
      "[137]\tvalidation_0-auc:0.713463\n",
      "[138]\tvalidation_0-auc:0.713617\n",
      "[139]\tvalidation_0-auc:0.7134\n",
      "[140]\tvalidation_0-auc:0.713556\n",
      "[141]\tvalidation_0-auc:0.713467\n",
      "[142]\tvalidation_0-auc:0.713747\n",
      "[143]\tvalidation_0-auc:0.713602\n",
      "[144]\tvalidation_0-auc:0.713685\n",
      "[145]\tvalidation_0-auc:0.713628\n",
      "[146]\tvalidation_0-auc:0.713884\n",
      "[147]\tvalidation_0-auc:0.714052\n",
      "[148]\tvalidation_0-auc:0.714272\n",
      "[149]\tvalidation_0-auc:0.714116\n",
      "[150]\tvalidation_0-auc:0.714114\n",
      "[151]\tvalidation_0-auc:0.714079\n",
      "[152]\tvalidation_0-auc:0.713961\n",
      "[153]\tvalidation_0-auc:0.713775\n",
      "[154]\tvalidation_0-auc:0.713465\n",
      "[155]\tvalidation_0-auc:0.71331\n",
      "[156]\tvalidation_0-auc:0.713675\n",
      "[157]\tvalidation_0-auc:0.713724\n",
      "[158]\tvalidation_0-auc:0.713734\n",
      "[159]\tvalidation_0-auc:0.713843\n",
      "[160]\tvalidation_0-auc:0.714278\n",
      "[161]\tvalidation_0-auc:0.71431\n",
      "[162]\tvalidation_0-auc:0.714397\n",
      "[163]\tvalidation_0-auc:0.714869\n",
      "[164]\tvalidation_0-auc:0.714911\n",
      "[165]\tvalidation_0-auc:0.715455\n",
      "[166]\tvalidation_0-auc:0.715536\n",
      "[167]\tvalidation_0-auc:0.715509\n",
      "[168]\tvalidation_0-auc:0.715692\n",
      "[169]\tvalidation_0-auc:0.715768\n",
      "[170]\tvalidation_0-auc:0.715997\n",
      "[171]\tvalidation_0-auc:0.715935\n",
      "[172]\tvalidation_0-auc:0.715944\n",
      "[173]\tvalidation_0-auc:0.716114\n",
      "[174]\tvalidation_0-auc:0.716244\n",
      "[175]\tvalidation_0-auc:0.716235\n",
      "[176]\tvalidation_0-auc:0.71649\n",
      "[177]\tvalidation_0-auc:0.716674\n",
      "[178]\tvalidation_0-auc:0.716875\n",
      "[179]\tvalidation_0-auc:0.717034\n",
      "[180]\tvalidation_0-auc:0.716986\n",
      "[181]\tvalidation_0-auc:0.71701\n",
      "[182]\tvalidation_0-auc:0.717003\n",
      "[183]\tvalidation_0-auc:0.716984\n",
      "[184]\tvalidation_0-auc:0.716824\n",
      "[185]\tvalidation_0-auc:0.716977\n",
      "[186]\tvalidation_0-auc:0.716979\n",
      "[187]\tvalidation_0-auc:0.716651\n",
      "[188]\tvalidation_0-auc:0.71665\n",
      "[189]\tvalidation_0-auc:0.716738\n",
      "[190]\tvalidation_0-auc:0.71657\n",
      "[191]\tvalidation_0-auc:0.716771\n",
      "[192]\tvalidation_0-auc:0.716995\n",
      "[193]\tvalidation_0-auc:0.717052\n",
      "[194]\tvalidation_0-auc:0.717058\n",
      "[195]\tvalidation_0-auc:0.717077\n",
      "[196]\tvalidation_0-auc:0.716905\n",
      "[197]\tvalidation_0-auc:0.716951\n",
      "[198]\tvalidation_0-auc:0.71706\n",
      "[199]\tvalidation_0-auc:0.717095\n",
      "[200]\tvalidation_0-auc:0.717431\n",
      "[201]\tvalidation_0-auc:0.71726\n",
      "[202]\tvalidation_0-auc:0.717394\n",
      "[203]\tvalidation_0-auc:0.717291\n",
      "[204]\tvalidation_0-auc:0.717439\n",
      "[205]\tvalidation_0-auc:0.717458\n",
      "[206]\tvalidation_0-auc:0.717346\n",
      "[207]\tvalidation_0-auc:0.717635\n",
      "[208]\tvalidation_0-auc:0.717724\n",
      "[209]\tvalidation_0-auc:0.718045\n",
      "[210]\tvalidation_0-auc:0.718138\n",
      "[211]\tvalidation_0-auc:0.718167\n",
      "[212]\tvalidation_0-auc:0.718088\n",
      "[213]\tvalidation_0-auc:0.718007\n",
      "[214]\tvalidation_0-auc:0.718004\n",
      "[215]\tvalidation_0-auc:0.71807\n",
      "[216]\tvalidation_0-auc:0.717772\n",
      "[217]\tvalidation_0-auc:0.717803\n",
      "[218]\tvalidation_0-auc:0.717618\n",
      "[219]\tvalidation_0-auc:0.717654\n",
      "[220]\tvalidation_0-auc:0.717873\n",
      "[221]\tvalidation_0-auc:0.718047\n",
      "[222]\tvalidation_0-auc:0.717911\n",
      "[223]\tvalidation_0-auc:0.717994\n",
      "[224]\tvalidation_0-auc:0.717981\n",
      "[225]\tvalidation_0-auc:0.717811\n",
      "[226]\tvalidation_0-auc:0.717449\n",
      "[227]\tvalidation_0-auc:0.71736\n",
      "[228]\tvalidation_0-auc:0.717143\n",
      "[229]\tvalidation_0-auc:0.717255\n",
      "[230]\tvalidation_0-auc:0.717435\n",
      "[231]\tvalidation_0-auc:0.71755\n",
      "Stopping. Best iteration:\n",
      "[211]\tvalidation_0-auc:0.718167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, early_stopping_rounds=20, eval_metric=\"auc\", eval_set=[(X_valid, y_valid)])\n",
    "predict_valid = model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71754996468434307"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cutoff = np.linspace(0.1, 0.3, 19)\n",
    "# cutoff_metrics(cutoff, predict_valid)\n",
    "roc_auc_score(y_valid, predict_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = transform(train, test, target, \n",
    "                        encode=ENCODE, scaling=True, fill_num=FILL_NUM, \n",
    "                        hc_drop=HC_DROP, high_cardinality=HC, eb_k=HC_K, eb_f=HC_F,\n",
    "                        to_drop=['Var214', 'Var220', 'Var222'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, target)\n",
    "predict_proba = model.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save submission\n",
    "# sub = pd.concat([test_id, pd.DataFrame(np.where(predict1==0, -1, 1), columns=['result'])], axis=1)\n",
    "sub = pd.concat([test_id, pd.DataFrame(predict_proba, columns=['result'])], axis=1)\n",
    "sub.to_csv('./input/sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train)\n",
    "# predict_proba = model.predict_proba(X_valid)[:, 1]\n",
    "# fpr, tpr, _ = roc_curve(y_valid, predict_proba)\n",
    "# roc_auc_score(y_valid, predict_proba)\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.plot([0,1],[0,1],'r--')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}