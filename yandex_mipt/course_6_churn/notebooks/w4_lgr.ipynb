{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score, roc_curve, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, Imputer, LabelBinarizer\n",
    "import category_encoders\n",
    "\n",
    "import time \n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X, y, model, kf):\n",
    "    X, y = np.array(X), np.array(y).reshape(-1)\n",
    "    cv_scores = np.zeros((5,4), dtype=np.float32)\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "\n",
    "        print( \"Fold \", i)\n",
    "\n",
    "        y_train, y_val = y[train_index].copy(), y[val_index].copy()\n",
    "        X_train, X_val = X[train_index, :].copy(), X[val_index, :].copy()\n",
    "        \n",
    "        fit_model = model.fit(X_train, y_train)\n",
    "        pred = fit_model.predict(X_val)\n",
    "\n",
    "        cv_scores[i, :] = [f1_score(y_val, pred), \n",
    "                           precision_score(y_val, pred), \n",
    "                           recall_score(y_val, pred), \n",
    "                           roc_auc_score(y_val, pred)]\n",
    "        \n",
    "    return cv_scores\n",
    "\n",
    "def print_metrics(cv_scores):\n",
    "    metrics = ['f1', 'precision', 'recall', 'roc auc']\n",
    "    cvmean = cv_scores.mean(0)\n",
    "    for i in range(4):\n",
    "        print(\"{} = {:.5f}\".format(metrics[i], cvmean[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./input/orange_small_churn_test_data.csv')\n",
    "train, target = pd.read_csv('./input/orange_small_churn_data.train'), \\\n",
    "np.where(pd.read_csv('./input/orange_small_churn_labels.train', header=-1)==1, 1, 0).ravel()\n",
    "\n",
    "test_id= test['ID']\n",
    "test.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check duplicates \n",
    "# # rows\n",
    "# np.sum(train.duplicated()) \n",
    "# # columns\n",
    "# arr = np.array(train)\n",
    "# dup = []\n",
    "# for i in range(arr.shape[1]):\n",
    "#     for j in range(i+1, arr.shape[1]):\n",
    "#         if np.all(np.equal(arr[:, i], arr[:, j])):\n",
    "#             dup.append((i, j))\n",
    "# dup # [(197, 219), (197, 221), (199, 213), (219, 221)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(train, test, target, to_drop=False, \n",
    "              high_cardinality=\"smoothing\", hc_treshold = 10, hc_drop=False, # high cardinality categorical\n",
    "              eb_k=50, eb_f=10,  # parameters for hc smoothing function \n",
    "              encode=False,  # categorical \n",
    "              fill_num=-1, scaling=False  # continuous \n",
    "             ):\n",
    "    \n",
    "    \"\"\" \n",
    "    data preprocessing \n",
    "    \n",
    "    :train, test: pandas DataFrame\n",
    "    :high_cardinality: way to handle categorical features with high number of levels\n",
    "    :encode: category encoding, 'ohe' = one hot, 'bin' = binary\n",
    "    :fill_num: fill nan for continuous features, -1 = with -1, ('mean', 'median') = strategy\n",
    "    :scaling: 'standard' = StandartScaler\n",
    "    \n",
    "    category features should have type 'object'\n",
    "    \"\"\"\n",
    "\n",
    "    # remove duplicates \n",
    "    if to_drop:\n",
    "        train = train.drop(to_drop, axis=1)\n",
    "        test = test.drop(to_drop, axis=1)\n",
    "    \n",
    "    ######## categorical features \n",
    "    \n",
    "    cat_features = train.columns[train.dtypes=='object']\n",
    "    num_features = train.columns[train.dtypes!='object']      \n",
    "        \n",
    "    # factorize \n",
    "    le = LabelEncoder()\n",
    "    train[cat_features] = train[cat_features].fillna('-1')\n",
    "    test[cat_features] = test[cat_features].fillna('-1')\n",
    "    for c in cat_features:\n",
    "        data=train[c].append(test[c])\n",
    "        le.fit(data.values.tolist())  # nan = 0 level\n",
    "        train[c] = le.transform(train[c].values.tolist())\n",
    "        test[c] = le.transform(test[c].values.tolist())       \n",
    "    \n",
    "    # mark nan with -1, if encoding not necessary \n",
    "    if not encode:\n",
    "        train[cat_features] = train[cat_features].replace(0, -1)\n",
    "        test[cat_features] = test[cat_features].replace(0, -1)        \n",
    "    ######## high cardinality\n",
    "    \n",
    "    if high_cardinality:\n",
    "\n",
    "        hc_features = train[cat_features].columns[train[cat_features].apply(lambda x: len(x.value_counts())) > hc_treshold]\n",
    "        target_mean = target.mean()\n",
    "        S = {}\n",
    "\n",
    "        for c in hc_features:\n",
    "\n",
    "            if high_cardinality == \"sr\":\n",
    "                # supervised ratio \n",
    "                group_means = pd.concat([train[c], pd.DataFrame(target, columns=['target'], index=train.index)], axis=1).groupby(c).mean()\n",
    "                group_means = group_means.target.to_dict()\n",
    "                for group in train[c].value_counts().index:\n",
    "                    S[group] = group_means[group]\n",
    "\n",
    "            if high_cardinality==\"woe\":\n",
    "                # weight of evidence\n",
    "                group_y1 = pd.concat([train[c], pd.DataFrame(target, columns=['target'], index=train.index)], axis=1).\\\n",
    "                groupby([c]).agg('sum')\n",
    "                group_y0 = pd.concat([train[c], pd.DataFrame(target, columns=['target'], index=train.index)], axis=1).\\\n",
    "                groupby([c]).agg('count') - group_y1\n",
    "                y1 = (target==1).sum()\n",
    "                y0 = (target==0).sum()\n",
    "                woe = np.log(((group_y1) / y1) / ((group_y0) / y0))\n",
    "                for i,v in zip(woe.index, np.where(np.isinf(woe), 0, woe)):\n",
    "                    S[i] = v[0]\n",
    "\n",
    "            if high_cardinality==\"smoothing\":\n",
    "                # empirical bayes (smoothing for small group)\n",
    "                group_means = pd.concat([train[c], pd.DataFrame(target, columns=['target'], index=train.index)], axis=1).groupby(c).mean()\n",
    "                group_means = group_means.target.to_dict()\n",
    "                group_counts = pd.concat([train[c], pd.DataFrame(target, columns=['target'], index=train.index)], axis=1).groupby(c).agg('count')\n",
    "                group_counts = group_counts.target.to_dict()\n",
    "\n",
    "                def smoothing_function(n, k, f):\n",
    "                    return 1 / (1 + np.exp(-(n-k)/f))\n",
    "\n",
    "                for group in train[c].value_counts().index:\n",
    "                    lam = smoothing_function(n=group_counts[group], k=eb_k, f=eb_f)\n",
    "                    S[group] = lam*group_means[group] + (1-lam)*target_mean\n",
    "\n",
    "            # transform train\n",
    "            train[c+'_avg'] = train[c].apply(lambda x: S[x]).copy()\n",
    "\n",
    "            # transform test\n",
    "            def hc_transform_test(x):\n",
    "                if x in S: \n",
    "                    return S[x]\n",
    "                else:\n",
    "                    return target_mean\n",
    "\n",
    "            test[c+'_avg'] = test[c].apply(hc_transform_test).copy()\n",
    "\n",
    "        # drop hc features \n",
    "        if hc_drop:\n",
    "            train.drop(hc_features, axis=1, inplace=True)\n",
    "            test.drop(hc_features, axis=1, inplace=True)\n",
    "\n",
    "        # update cat features \n",
    "        cat_features = sorted(list(set(cat_features).difference(hc_features)))\n",
    "\n",
    "    ######## for linear models \n",
    "    \n",
    "    # fill missings\n",
    "    if fill_num in ['mean', 'median']:\n",
    "        imputer = Imputer(strategy=fill_num)\n",
    "        train[num_features] = imputer.fit_transform(train[num_features])\n",
    "        test[num_features] = imputer.transform(test[num_features])\n",
    "    elif fill_num < 0:\n",
    "        train[num_features] = train[num_features].fillna(fill_num)\n",
    "        test[num_features] = test[num_features].fillna(fill_num)\n",
    "        \n",
    "    # scaling\n",
    "    if scaling=='standard':\n",
    "        scaler = StandardScaler()\n",
    "        train[num_features] = scaler.fit_transform(train[num_features])\n",
    "        test[num_features] = scaler.transform(test[num_features])\n",
    "    \n",
    "    ######## encoding \n",
    "    if encode=='ohe':\n",
    "        # one hot encoding, memory inefficient\n",
    "        oh = OneHotEncoder(sparse=False)\n",
    "        for c in cat_features:\n",
    "            data=train[c].append(test[c])\n",
    "            oh.fit(data.reshape(-1,1))            \n",
    "            train_temp = oh.transform(train[c].reshape(-1,1))\n",
    "            test_temp = oh.transform(test[c].reshape(-1,1))\n",
    "            train = pd.concat([train, pd.DataFrame(train_temp, \n",
    "                                                   columns=[(c+\"_\"+str(i)) for i in data.value_counts().index],\n",
    "                                                   index = train.index\n",
    "                                                  )], axis=1)\n",
    "            test = pd.concat([test, pd.DataFrame(test_temp, \n",
    "                                                 columns=[(c+\"_\"+str(i)) for i in data.value_counts().index],\n",
    "                                                 index = test.index\n",
    "                                                )], axis=1)\n",
    "            # drop column\n",
    "            train.drop(c, axis=1, inplace=True)\n",
    "            test.drop(c, axis=1, inplace=True)\n",
    "    \n",
    "    if encode=='bin':\n",
    "        # binary encoding \n",
    "        pass\n",
    "            \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = transform(train, test, target, \n",
    "#                         encode='ohe', scaling=True, fill_num='median', hc_drop=True,\n",
    "#                         to_drop=['Var214', 'Var220', 'Var222'])\n",
    "# train.shape, test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.isnull().sum().value_counts()[:5]\n",
    "# nan_group_1 = X[X.columns[X.isnull().sum() == 34506]].dropna().index\n",
    "# nan_group_2 = X[X.columns[X.isnull().sum() == 34141]].dropna().index\n",
    "# nan_group_3 = X[X.columns[X.isnull().sum() == 33975]].dropna().index\n",
    "# nan_group_4 = X[X.columns[X.isnull().sum() == 33881]].dropna().index\n",
    "# set(nan_group_3).intersection(list(nan_group_2))\n",
    "# выставим порог отсева пустых значений на 99%\n",
    "# X = X[X.columns[(X.isnull().sum() / X.shape[0] ) < 0.75]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\dev\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:128: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "C:\\dev\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:129: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "C:\\dev\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:130: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 122) (10000, 122)\n",
      "Fold  0\n",
      "Fold  1\n",
      "Fold  2\n",
      "Fold  3\n",
      "Fold  4\n",
      "f1 = 0.19412\n",
      "precision = 0.11515\n",
      "recall = 0.61828\n",
      "roc auc = 0.61804\n"
     ]
    }
   ],
   "source": [
    "MISS_THERSHOLD = 0.9\n",
    "ENCODE = \"ohe\"\n",
    "FILL_NUM = 'median'\n",
    "HC = \"woe\"\n",
    "HC_DROP = True\n",
    "HC_K = 50\n",
    "HC_F = 10\n",
    "\n",
    "############################################################################################################\n",
    "# load \n",
    "test = pd.read_csv('./input/orange_small_churn_test_data.csv')\n",
    "train, target = pd.read_csv('./input/orange_small_churn_data.train'), \\\n",
    "np.where(pd.read_csv('./input/orange_small_churn_labels.train', header=-1)==1, 1, 0).ravel()\n",
    "\n",
    "test_id= test['ID']\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "# выставим порог отсева пустых значений на 95%\n",
    "to_drop = train.columns[(train.isnull().sum() / train.shape[0] ) >= MISS_THERSHOLD]\n",
    "train.drop(to_drop, axis=1, inplace=True)\n",
    "test.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# transform\n",
    "train, test = transform(train, test, target, \n",
    "                        encode=ENCODE, scaling=True, fill_num=FILL_NUM, \n",
    "                        hc_drop=HC_DROP, high_cardinality=HC, eb_k=HC_K, eb_f=HC_F,\n",
    "                        to_drop=['Var214', 'Var220', 'Var222'])\n",
    "print(train.shape, test.shape )\n",
    "\n",
    "# split data \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=5000, random_state=42, stratify=target)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# cv \n",
    "cv_scores = cross_val(X_train, y_train, model, kf)\n",
    "print_metrics(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba = model.predict_proba(X_valid)[:, 1]\n",
    "# fpr, tpr, _ = roc_curve(y_valid, predict_proba)\n",
    "# roc_auc_score(y_valid, predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(fpr, tpr)\n",
    "# plt.plot([0,1],[0,1],'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff = np.linspace(0.45, 0.6, 15)\n",
    "# for cut in cutoff:\n",
    "#     print(\"f1: {:.3}\\trecall: {:.3}\\tprecision: {:.3}\\tauc: {:.3}\\tacc: {:.3}\\tcutoff: {:.2} \".format(\n",
    "#         f1_score(y_valid, predict_proba > cut), \n",
    "#         recall_score(y_valid, predict_proba > cut), \n",
    "#         precision_score(y_valid, predict_proba > cut), \n",
    "#         roc_auc_score(y_valid, predict_proba > cut),\n",
    "#         accuracy_score(y_valid, predict_proba>cut),\n",
    "#         cut\n",
    "#     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save submission\n",
    "sub = pd.concat([test_id, pd.DataFrame(np.where(predict==0, -1, 1), columns=['result'])], axis=1)\n",
    "sub.to_csv('./input/sub_lr.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
